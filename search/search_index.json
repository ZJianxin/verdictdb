{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"VerdictDB Documentation \u00b6 Introduction \u00b6 VerdictDB is a thin, platform-independent, interactive analytics library that works on top of your existing (or backend) database system (e.g., MySQL, PostgreSQL, Redshift, etc.). For platform-independence, VerdictDB makes all communications with the backend database in SQL. For interactive querying, VerdictDB intelligently infers the query answers based on the results processed on a part of the original data. Those inferred answers are highly accurate estimators of the exact answers. Furthermore, even when only exact answers are needed , VerdictDB can also be useful with its streaming sql engine . The streaming sql engine provides interactive-speed feedbacks in the process of computing exact answers. See this page if you want to know more about VerdictDB's internal mechanism. Workflow Overview \u00b6 First, users must create a scramble for their large table. The scramble is just some table in a special format. Once the scramble is created, VerdictDB performs its unique operations to quickly process aggregate queries involving the large table. Contents \u00b6 Getting Started Quickstart (Java) Quickstart (Python) Install / Download What's More How VerdictDB works Basics Architecture Query Processing Tutorial Setting up databases MySQL Apache Spark Setting up TPC-H data Example Applications MySQL Apache Spark Reference Connecting to Data Sources VerdictDB JDBC Properties Creating/Viewing Scrambles Appending Scrambles Dropping Scrambles Select-Query Syntax Stream Querying Javadoc License and Developments \u00b6 VerdictDB is under the Apache License 2.0 ; thus, it is completely free for both commercial and non-commercial purposes. VerdictDB is developed by the database group at the University of Michigan, Ann Arbor.","title":"Home"},{"location":"#verdictdb-documentation","text":"","title":"VerdictDB Documentation"},{"location":"#introduction","text":"VerdictDB is a thin, platform-independent, interactive analytics library that works on top of your existing (or backend) database system (e.g., MySQL, PostgreSQL, Redshift, etc.). For platform-independence, VerdictDB makes all communications with the backend database in SQL. For interactive querying, VerdictDB intelligently infers the query answers based on the results processed on a part of the original data. Those inferred answers are highly accurate estimators of the exact answers. Furthermore, even when only exact answers are needed , VerdictDB can also be useful with its streaming sql engine . The streaming sql engine provides interactive-speed feedbacks in the process of computing exact answers. See this page if you want to know more about VerdictDB's internal mechanism.","title":"Introduction"},{"location":"#workflow-overview","text":"First, users must create a scramble for their large table. The scramble is just some table in a special format. Once the scramble is created, VerdictDB performs its unique operations to quickly process aggregate queries involving the large table.","title":"Workflow Overview"},{"location":"#contents","text":"Getting Started Quickstart (Java) Quickstart (Python) Install / Download What's More How VerdictDB works Basics Architecture Query Processing Tutorial Setting up databases MySQL Apache Spark Setting up TPC-H data Example Applications MySQL Apache Spark Reference Connecting to Data Sources VerdictDB JDBC Properties Creating/Viewing Scrambles Appending Scrambles Dropping Scrambles Select-Query Syntax Stream Querying Javadoc","title":"Contents"},{"location":"#license-and-developments","text":"VerdictDB is under the Apache License 2.0 ; thus, it is completely free for both commercial and non-commercial purposes. VerdictDB is developed by the database group at the University of Michigan, Ann Arbor.","title":"License and Developments"},{"location":"getting_started/install/","text":"Download/Install \u00b6 Please visit this page for download and installation.","title":"Download / Install"},{"location":"getting_started/install/#downloadinstall","text":"Please visit this page for download and installation.","title":"Download/Install"},{"location":"getting_started/overview/","text":"","title":"Overview"},{"location":"getting_started/quickstart/","text":"Quickstart Guide (Java) \u00b6 We will install VerdictDB, create a connection, and issue a simple query to VerdictDB. In this Quickstart Guide, we will use MySQL for VerdictDB's backend database. See How to Connect for the examples of connecting to other databases. Install \u00b6 Create an empty Maven project and place the following dependency in the <dependencies> of your pom.xml. <dependency> <groupId>org.verdictdb</groupId> <artifactId>verdictdb-core</artifactId> <version>0.5.6</version> </dependency> To use MySQL, add the following entry as well: <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <version>5.1.46</version> </dependency> Insert Data \u00b6 We will first generate small data to play with. // Suppose username is root and password is rootpassword. Connection mysqlConn = DriverManager . getConnection ( \"jdbc:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement stmt = mysqlConn . createStatement (); stmt . execute ( \"create schema myschema\" ); stmt . execute ( \"create table myschema.sales (\" + \" product varchar(100),\" + \" price double)\" ); // insert 1000 rows List < String > productList = Arrays . asList ( \"milk\" , \"egg\" , \"juice\" ); for ( int i = 0 ; i < 1000 ; i ++) { int randInt = ThreadLocalRandom . current (). nextInt ( 0 , 3 ); String product = productList . get ( randInt ); double price = ( randInt + 2 ) * 10 + ThreadLocalRandom . current (). nextInt ( 0 , 10 ); stmt . execute ( String . format ( \"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\" , product , price )); } Test VerdictDB \u00b6 Create a JDBC connection to VerdictDB. Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement vstmt = verdict . createStatement (); Create a special table called a \"scramble\", which is the replica of the original table with extra information VerdictDB uses for speeding up query processing. vstmt . execute ( \"create scramble myschema.sales_scrambled from myschema.sales\" ); Run just a regular query to the original table. ResultSet rs = vstmt . executeQuery ( \"select product, avg(price) \" + \"from myschema.sales_scrambled \" + \"group by product \" + \"order by product\" ); Internally, VerdictDB rewrites the above query to use the scramble. It is equivalent to explicitly specifying the scramble in the from clause of the above query. Complete Example Java File \u00b6 (Yongjoo: update this according to the above code) import java.sql.* ; import java.util.ArrayList ; import java.util.Arrays ; import java.util.List ; import java.util.concurrent.ThreadLocalRandom ; public class FirstVerdictDBExample { public static void main ( String args []) throws SQLException { // Suppose username is root and password is rootpassword. Connection mysqlConn = DriverManager . getConnection ( \"jdbc:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement stmt = mysqlConn . createStatement (); stmt . execute ( \"create schema myschema\" ); stmt . execute ( \"create table myschema.sales (\" + \" product varchar(100),\" + \" price double)\" ); // insert 1000 rows List < String > productList = Arrays . asList ( \"milk\" , \"egg\" , \"juice\" ); for ( int i = 0 ; i < 1000 ; i ++) { int randInt = ThreadLocalRandom . current (). nextInt ( 0 , 3 ) String product = productList . get ( randInt ); double price = ( randInt + 2 ) * 10 + ThreadLocalRandom . current (). nextInt ( 0 , 10 ); stmt . execute ( String . format ( \"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\" , product , price )); } Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement vstmt = verdict . createStatement (); // Use CREATE SCRAMBLE syntax to create scrambled tables. vstmt . execute ( \"create scramble myschema.sales_scrambled from myschema.sales\" ); ResultSet rs = vstmt . executeQuery ( \"select product, avg(price) \" + \"from myschema.sales_scrambled \" + \"group by product \" + \"order by product\" ); // Do something after getting the results. } }","title":"Quickstart (Java)"},{"location":"getting_started/quickstart/#quickstart-guide-java","text":"We will install VerdictDB, create a connection, and issue a simple query to VerdictDB. In this Quickstart Guide, we will use MySQL for VerdictDB's backend database. See How to Connect for the examples of connecting to other databases.","title":"Quickstart Guide (Java)"},{"location":"getting_started/quickstart/#install","text":"Create an empty Maven project and place the following dependency in the <dependencies> of your pom.xml. <dependency> <groupId>org.verdictdb</groupId> <artifactId>verdictdb-core</artifactId> <version>0.5.6</version> </dependency> To use MySQL, add the following entry as well: <dependency> <groupId>mysql</groupId> <artifactId>mysql-connector-java</artifactId> <version>5.1.46</version> </dependency>","title":"Install"},{"location":"getting_started/quickstart/#insert-data","text":"We will first generate small data to play with. // Suppose username is root and password is rootpassword. Connection mysqlConn = DriverManager . getConnection ( \"jdbc:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement stmt = mysqlConn . createStatement (); stmt . execute ( \"create schema myschema\" ); stmt . execute ( \"create table myschema.sales (\" + \" product varchar(100),\" + \" price double)\" ); // insert 1000 rows List < String > productList = Arrays . asList ( \"milk\" , \"egg\" , \"juice\" ); for ( int i = 0 ; i < 1000 ; i ++) { int randInt = ThreadLocalRandom . current (). nextInt ( 0 , 3 ); String product = productList . get ( randInt ); double price = ( randInt + 2 ) * 10 + ThreadLocalRandom . current (). nextInt ( 0 , 10 ); stmt . execute ( String . format ( \"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\" , product , price )); }","title":"Insert Data"},{"location":"getting_started/quickstart/#test-verdictdb","text":"Create a JDBC connection to VerdictDB. Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement vstmt = verdict . createStatement (); Create a special table called a \"scramble\", which is the replica of the original table with extra information VerdictDB uses for speeding up query processing. vstmt . execute ( \"create scramble myschema.sales_scrambled from myschema.sales\" ); Run just a regular query to the original table. ResultSet rs = vstmt . executeQuery ( \"select product, avg(price) \" + \"from myschema.sales_scrambled \" + \"group by product \" + \"order by product\" ); Internally, VerdictDB rewrites the above query to use the scramble. It is equivalent to explicitly specifying the scramble in the from clause of the above query.","title":"Test VerdictDB"},{"location":"getting_started/quickstart/#complete-example-java-file","text":"(Yongjoo: update this according to the above code) import java.sql.* ; import java.util.ArrayList ; import java.util.Arrays ; import java.util.List ; import java.util.concurrent.ThreadLocalRandom ; public class FirstVerdictDBExample { public static void main ( String args []) throws SQLException { // Suppose username is root and password is rootpassword. Connection mysqlConn = DriverManager . getConnection ( \"jdbc:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement stmt = mysqlConn . createStatement (); stmt . execute ( \"create schema myschema\" ); stmt . execute ( \"create table myschema.sales (\" + \" product varchar(100),\" + \" price double)\" ); // insert 1000 rows List < String > productList = Arrays . asList ( \"milk\" , \"egg\" , \"juice\" ); for ( int i = 0 ; i < 1000 ; i ++) { int randInt = ThreadLocalRandom . current (). nextInt ( 0 , 3 ) String product = productList . get ( randInt ); double price = ( randInt + 2 ) * 10 + ThreadLocalRandom . current (). nextInt ( 0 , 10 ); stmt . execute ( String . format ( \"INSERT INTO myschema.sales (product, price) VALUES('%s', %.0f)\" , product , price )); } Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement vstmt = verdict . createStatement (); // Use CREATE SCRAMBLE syntax to create scrambled tables. vstmt . execute ( \"create scramble myschema.sales_scrambled from myschema.sales\" ); ResultSet rs = vstmt . executeQuery ( \"select product, avg(price) \" + \"from myschema.sales_scrambled \" + \"group by product \" + \"order by product\" ); // Do something after getting the results. } }","title":"Complete Example Java File"},{"location":"getting_started/quickstart_python/","text":"Quickstart Guide (Python) \u00b6 We will download/install pyverdict (a Python interface to VerdictDB) and issues a simple query. In this Quickstart Guide, we will use MySQL for VerdictDB's backend database. Install \u00b6 pyverdict is distributed with PyPI. Use the following command for installation. pip install pyverdict or pip install pyverdict --upgrade Note: Prerequisites pyverdict requires miniconda for Python 3.7, which can be installed for local users (i.e., without sudo access). Connect \u00b6 Suppose MySQL is set as described on this page . import pyverdict verdict = pyverdict . mysql ( 'localhost' , 'root' , '' ) verdict . sql ( 'show schemas' ) # this returns pandas DataFrame containing schema names","title":"Quickstart (Python)"},{"location":"getting_started/quickstart_python/#quickstart-guide-python","text":"We will download/install pyverdict (a Python interface to VerdictDB) and issues a simple query. In this Quickstart Guide, we will use MySQL for VerdictDB's backend database.","title":"Quickstart Guide (Python)"},{"location":"getting_started/quickstart_python/#install","text":"pyverdict is distributed with PyPI. Use the following command for installation. pip install pyverdict or pip install pyverdict --upgrade Note: Prerequisites pyverdict requires miniconda for Python 3.7, which can be installed for local users (i.e., without sudo access).","title":"Install"},{"location":"getting_started/quickstart_python/#connect","text":"Suppose MySQL is set as described on this page . import pyverdict verdict = pyverdict . mysql ( 'localhost' , 'root' , '' ) verdict . sql ( 'show schemas' ) # this returns pandas DataFrame containing schema names","title":"Connect"},{"location":"getting_started/results/","text":"Reading Query Results \u00b6 JDBC interface \u00b6 Regular query \u00b6 Streaming query \u00b6 VerdictContext interface \u00b6 sql() method \u00b6 streamsql() method \u00b6","title":"Reading Query Results"},{"location":"getting_started/results/#reading-query-results","text":"","title":"Reading Query Results"},{"location":"getting_started/results/#jdbc-interface","text":"","title":"JDBC interface"},{"location":"getting_started/results/#regular-query","text":"","title":"Regular query"},{"location":"getting_started/results/#streaming-query","text":"","title":"Streaming query"},{"location":"getting_started/results/#verdictcontext-interface","text":"","title":"VerdictContext interface"},{"location":"getting_started/results/#sql-method","text":"","title":"sql() method"},{"location":"getting_started/results/#streamsql-method","text":"","title":"streamsql() method"},{"location":"getting_started/supported_database/","text":"Supported Databases \u00b6 MySQL \u00b6 PostgreSQL \u00b6","title":"Supported database"},{"location":"getting_started/supported_database/#supported-databases","text":"","title":"Supported Databases"},{"location":"getting_started/supported_database/#mysql","text":"","title":"MySQL"},{"location":"getting_started/supported_database/#postgresql","text":"","title":"PostgreSQL"},{"location":"getting_started/whatsmore/","text":"What's More \u00b6 Connecting to Data Sources \u00b6 VerdictDB JDBC Properties \u00b6 Scrambles \u00b6 What is a Scramble? Creating/Viewing Scrambles Dropping Scrambles Simple Querying \u00b6 Select Query Syntax \u00b6 Stream Querying \u00b6 Javadoc \u00b6","title":"What's More"},{"location":"getting_started/whatsmore/#whats-more","text":"","title":"What's More"},{"location":"getting_started/whatsmore/#connecting-to-data-sources","text":"","title":"Connecting to Data Sources"},{"location":"getting_started/whatsmore/#verdictdb-jdbc-properties","text":"","title":"VerdictDB JDBC Properties"},{"location":"getting_started/whatsmore/#scrambles","text":"What is a Scramble? Creating/Viewing Scrambles Dropping Scrambles","title":"Scrambles"},{"location":"getting_started/whatsmore/#simple-querying","text":"","title":"Simple Querying"},{"location":"getting_started/whatsmore/#select-query-syntax","text":"","title":"Select Query Syntax"},{"location":"getting_started/whatsmore/#stream-querying","text":"","title":"Stream Querying"},{"location":"getting_started/whatsmore/#javadoc","text":"","title":"Javadoc"},{"location":"how_it_works/architecture/","text":"Architecture \u00b6 Deployment \u00b6 VerdictDB is a thin Java library placed between the user (or an application) and the backend database. Its Python library, i.e., pyverdict , also relies on the Java library. The user can make a connection to VerdictDB either using the standard JDBC or the public API provided by VerdictDB. VerdictDB communicates with the backend database also using JDBC except for special cases (e.g., Apache Spark's SparkSession). Internal Architecture \u00b6 VerdictDB has the following hierarchical structure. In the list below, an item that comes earlier in the following list depends (only) on the item that comes after the item. For example, JDBC Wrapper relies on VerdictContext. However, JDBC Wrapper does not directly rely on ExecutionContext. JDBC Wrapper (public interface) VerdictContext (public interface) ExecutionContext QueryCoordinator ExecutionPlan ExecutionNode Connection (VerdictContext) \u00b6 When the user makes a connection to VerdictDB using VerdictContext. VerdictDB's JDBC interface is simply a wrapper over VerdictContext. VerdictContext contains the information sufficient to connect to the backend database. If the user wishes VerdictDB to connect to the backend database using the JDBC interface, it is recommended to pass a connection string; then, VerdictDB creates multiple (10 by default) connections for concurrent query executions. The concurrent query executions make it possible to interleave short, light-weight queries (e.g., metadata retrieval) among long, data-intensive queries (e.g., aggregations). Execution (ExecutionContext) \u00b6 When the user issues a query, VerdictDB lets VerdictContext create a new instance of ExecutionContext. The instance of ExecutionContext is responsible for the execution of the single query. After the query execution is finished, the ExecutionContext instance is removed from VerdictContext's ExecutionContext list. JVM then may garbage-collect it. For the query execution, ExecutionContext creates a different type of QueryCoordinator instance, as describe below. Actual Query Processing (QueryCoordinator) \u00b6 Depending on the query type, a different type of QueryCoordinator is created. An important type (or class) is SelectQueryCoordinator. SelectQueryCoordinator parses and converts a given select query into a Java object. The Java object represents relational operations in a platform-independent way (since different DBMS have different SQL syntaxes). Then, SelectQueryCoordinator plans how to execute the Java object. Finally, the plan is executed. See below for more details about the plan. Plan and Nodes (ExecutionPlan and ExecutionNode) \u00b6 VerdictDB's most internal query executions are based on ExecutionPlan and ExecutionNode. A few exceptions are simple metadata updates. Here, we describe the general concepts about plans and nodes. How the plans and nodes are created, given a select query, is described on this page . An ExecutionPlan instance is a directed acyclic graph that consists of one or more ExecutionNode instances. An ExecutionNode instance corresponds to a single select query without any subqueries (either projection or aggregation). The directed edge between two ExecutionNode instances indicates the dependency: the parent node depends on some information returned by the child node. Each node runs on a separate Java thread for maximum concurrency.","title":"Architecture"},{"location":"how_it_works/architecture/#architecture","text":"","title":"Architecture"},{"location":"how_it_works/architecture/#deployment","text":"VerdictDB is a thin Java library placed between the user (or an application) and the backend database. Its Python library, i.e., pyverdict , also relies on the Java library. The user can make a connection to VerdictDB either using the standard JDBC or the public API provided by VerdictDB. VerdictDB communicates with the backend database also using JDBC except for special cases (e.g., Apache Spark's SparkSession).","title":"Deployment"},{"location":"how_it_works/architecture/#internal-architecture","text":"VerdictDB has the following hierarchical structure. In the list below, an item that comes earlier in the following list depends (only) on the item that comes after the item. For example, JDBC Wrapper relies on VerdictContext. However, JDBC Wrapper does not directly rely on ExecutionContext. JDBC Wrapper (public interface) VerdictContext (public interface) ExecutionContext QueryCoordinator ExecutionPlan ExecutionNode","title":"Internal Architecture"},{"location":"how_it_works/architecture/#connection-verdictcontext","text":"When the user makes a connection to VerdictDB using VerdictContext. VerdictDB's JDBC interface is simply a wrapper over VerdictContext. VerdictContext contains the information sufficient to connect to the backend database. If the user wishes VerdictDB to connect to the backend database using the JDBC interface, it is recommended to pass a connection string; then, VerdictDB creates multiple (10 by default) connections for concurrent query executions. The concurrent query executions make it possible to interleave short, light-weight queries (e.g., metadata retrieval) among long, data-intensive queries (e.g., aggregations).","title":"Connection (VerdictContext)"},{"location":"how_it_works/architecture/#execution-executioncontext","text":"When the user issues a query, VerdictDB lets VerdictContext create a new instance of ExecutionContext. The instance of ExecutionContext is responsible for the execution of the single query. After the query execution is finished, the ExecutionContext instance is removed from VerdictContext's ExecutionContext list. JVM then may garbage-collect it. For the query execution, ExecutionContext creates a different type of QueryCoordinator instance, as describe below.","title":"Execution (ExecutionContext)"},{"location":"how_it_works/architecture/#actual-query-processing-querycoordinator","text":"Depending on the query type, a different type of QueryCoordinator is created. An important type (or class) is SelectQueryCoordinator. SelectQueryCoordinator parses and converts a given select query into a Java object. The Java object represents relational operations in a platform-independent way (since different DBMS have different SQL syntaxes). Then, SelectQueryCoordinator plans how to execute the Java object. Finally, the plan is executed. See below for more details about the plan.","title":"Actual Query Processing (QueryCoordinator)"},{"location":"how_it_works/architecture/#plan-and-nodes-executionplan-and-executionnode","text":"VerdictDB's most internal query executions are based on ExecutionPlan and ExecutionNode. A few exceptions are simple metadata updates. Here, we describe the general concepts about plans and nodes. How the plans and nodes are created, given a select query, is described on this page . An ExecutionPlan instance is a directed acyclic graph that consists of one or more ExecutionNode instances. An ExecutionNode instance corresponds to a single select query without any subqueries (either projection or aggregation). The directed edge between two ExecutionNode instances indicates the dependency: the parent node depends on some information returned by the child node. Each node runs on a separate Java thread for maximum concurrency.","title":"Plan and Nodes (ExecutionPlan and ExecutionNode)"},{"location":"how_it_works/basics/","text":"Basics \u00b6 The basic idea is simple: by pre-organizing data in a special way, your queries can run faster. This idea has been widely-used in the various databases, including traditional RDBMS and modern distributed SQL-on-Hadoop systems. The examples are indexing, columnar format, compression, etc. Our system, VerdictDB, uses a new orthogonal approach, which we call scrambling (more details follow shortly). Since our approach is orthogonal to the existing approaches (e.g., columnar formats and compressions), we can combine scrambling with other approaches as well; in fact, VerdictDB also uses columnar format and compression (as well as scrambling) for maximum possible speedups whenever possible. VerdictDB mainly focuses on speeding up aggregate queries, i.e., the queries including aggregate functions, such as sum , count , and avg . Unlike lookup-style queries, these queries typically require extensive data scans; thus, query latencies can be very slow especially when the size of data is large. Based on these simple aggregate queries, more advanced analytics can also be performed. We describe them on a separate article . Then, what is scrambling and how do queries run faster with it? We present conceptual ideas below. VerdictDB's deployment, user interface, and architecture are discussed on this page . More details about internal query processing logic is described on this page . Scrambling \u00b6 A scramble is a regular database table that consists of randomly shuffle tuples (of an original table) with an extra, augmented column named verdidctdbblock . Physically, a set of tuples associated with the same verdidctdbblock are clustered (using partition supported by most modern databases). VerdictDB creates a scramble when the user issues a create-scramble query . How do queries run faster \u00b6 To quickly produce the answers to aggregate queries, VerdictDB relies on a well-known statistical property called the law of large numbers . The law of large numbers indicates that many commonly-used statistics (e.g., mean) can be precisely estimated using a sample , a randomly chosen subset of the original data. Observe that a set of tuples associated with the verdictdbblock in a scramble amounts to a sample; thus, by processing one or just a few blocks, we can produce very accurate estimates of the exact answer, i.e., the answer you would get if the entire data is processed. In some situations, however, only the exact answers are needed, and you may think VerdictDB's approximate answers not directly usable. However, even in that case, VerdictDB still offers benefits with its streaming query engine . The streaming query engine progressively updates its estimates while the exact answer is being computed. With this mechanism, you know what your exact answers will be like even before all data is processed, while the more accurate answers are continuously computed in the background. If the estimates are much different from what you originally expected (e.g., wrong filtering conditions, wrong tables or columns specified, etc.), you can simply stop the execution in the middle. Find more details about the streaming query engine on this page . More technical details can be found in our research paper .","title":"Basics"},{"location":"how_it_works/basics/#basics","text":"The basic idea is simple: by pre-organizing data in a special way, your queries can run faster. This idea has been widely-used in the various databases, including traditional RDBMS and modern distributed SQL-on-Hadoop systems. The examples are indexing, columnar format, compression, etc. Our system, VerdictDB, uses a new orthogonal approach, which we call scrambling (more details follow shortly). Since our approach is orthogonal to the existing approaches (e.g., columnar formats and compressions), we can combine scrambling with other approaches as well; in fact, VerdictDB also uses columnar format and compression (as well as scrambling) for maximum possible speedups whenever possible. VerdictDB mainly focuses on speeding up aggregate queries, i.e., the queries including aggregate functions, such as sum , count , and avg . Unlike lookup-style queries, these queries typically require extensive data scans; thus, query latencies can be very slow especially when the size of data is large. Based on these simple aggregate queries, more advanced analytics can also be performed. We describe them on a separate article . Then, what is scrambling and how do queries run faster with it? We present conceptual ideas below. VerdictDB's deployment, user interface, and architecture are discussed on this page . More details about internal query processing logic is described on this page .","title":"Basics"},{"location":"how_it_works/basics/#scrambling","text":"A scramble is a regular database table that consists of randomly shuffle tuples (of an original table) with an extra, augmented column named verdidctdbblock . Physically, a set of tuples associated with the same verdidctdbblock are clustered (using partition supported by most modern databases). VerdictDB creates a scramble when the user issues a create-scramble query .","title":"Scrambling"},{"location":"how_it_works/basics/#how-do-queries-run-faster","text":"To quickly produce the answers to aggregate queries, VerdictDB relies on a well-known statistical property called the law of large numbers . The law of large numbers indicates that many commonly-used statistics (e.g., mean) can be precisely estimated using a sample , a randomly chosen subset of the original data. Observe that a set of tuples associated with the verdictdbblock in a scramble amounts to a sample; thus, by processing one or just a few blocks, we can produce very accurate estimates of the exact answer, i.e., the answer you would get if the entire data is processed. In some situations, however, only the exact answers are needed, and you may think VerdictDB's approximate answers not directly usable. However, even in that case, VerdictDB still offers benefits with its streaming query engine . The streaming query engine progressively updates its estimates while the exact answer is being computed. With this mechanism, you know what your exact answers will be like even before all data is processed, while the more accurate answers are continuously computed in the background. If the estimates are much different from what you originally expected (e.g., wrong filtering conditions, wrong tables or columns specified, etc.), you can simply stop the execution in the middle. Find more details about the streaming query engine on this page . More technical details can be found in our research paper .","title":"How do queries run faster"},{"location":"how_it_works/query_processing/","text":"Query Processing \u00b6 On this page, we describe how VerdictDB could speed up query processing. Note that for query processing, VerdictDB internally creates directed acyclic graph (DAG) representations (as described on this page ) and use it for processing queries. The key to VerdictDB's faster query processing is how the DAG is constructed, which we describe below. DAG Construction \u00b6 For description, we use the following example query: select product , avg ( sales_price ) as avg_price from ( select product , price * ( 1 - discount ) as sales_price from sales_table_scramble where order_date between date '2018-01-01' and date '2018-01-31' ) t group by product order by avg_price desc In the above example, the inner query (projection) computes the price after discount, i.e., sales_price , and then the outer query (aggregation) computes the average of sales_price . Although this example query may be flattened into a simpler form, we intentionally use this nested form to make our description more general. Also, although VerdictDB internally parses the query (in String format) into its internal Java objects, our description will keep using the query string for easier understanding. We suppose that a scramble sales_table_scramble has been created for the sales_table table, and sales_table_scramble contains three blocks. As described on this page , each block of the scramble amounts to a random sample of the original table, i.e., sales_table . Step 1: regular DAG construction \u00b6 The given query is decomposed into multiple queries, each of which is a flat query. Except for the root node, all other nodes include create table as select ... queries. The query for a parent node depends on its children. Below we depict the DAG and the queries for those nodes. -- Q1 select * from temp_table2 -- Q2 create table temp_table2 select product , avg ( sales_price ) as avg_price from temp_table1 t group by product order by avg_price desc -- Q3 create table temp_table1 select product , price * ( 1 - discount ) as sales_price from sales_table_scramble where order_date between date '2018-01-01' and date '2018-01-31' Step 2: progressive aggregation DAG construction \u00b6 VerdictDB converts a part of the DAG to enable progressive aggregations. The affected parts are the aggregate queries including scrambles in its from clause or the projections of scrambles. After the conversion, the DAG looks like below. First, each of the projection nodes at the bottom only involves a part of the scramble. In this simple example, the single scramble is split into three projections. See that the following query includes an extra filtering predicate, i.e., verdictdbblock = 0 , to only select the particular block. -- P1 create table temp_table1 select product , price * ( 1 - discount ) as sales_price from sales_table_scramble where order_date between date '2018-01-01' and date '2018-01-31' and verdictdbblock = 0 Second, the aggregation is separately computed for each of those projections. It is important to note that the original avg function was converted into two separate aggregate functions, i.e., sum and count . The avg function value will be restored later. -- A1 create table temp_table2 select product , sum ( sales_price ) as sum_price , count ( sales_price ) as count_price from temp_table1 t group by product ; Observe that the individual aggregation nodes (A1, A2, and A3) only involves its own verdictdbblock, i.e., identified with 0, 1, and 2. To compute the exact answers, we combine those individual aggregates using additional nodes Combiners (C1 and C2). Naturally, the number of the Combiners is always one fewer than the number of individual aggregate nodes. Suppose A1 creates a temporary table temp_table2 , A2 creates temp_table3 , and A3 creates temp_table4 . Then, the Combiners perform the operations as follows. -- C1 create table temp_table5 select product , sum ( sum_price ) as sum_price , count ( count_price ) as count_price from ( select * from temp_table2 union all select * from temp_table3 ) t group by product ; The nodes also propagate some necessary metadata about the processed verdictdbblocks thus far. Finally, the node S collects those aggregates, scale them appropriately, and restore the original select items. -- S create table temp_table7 select product , ( 3 . 0 * sum_price ) / ( 3 . 0 * count_price ) as avg_price from temp_table5 group by product ; In the above query (for the node S), 3.0 * sum_price is an unbiased estimator for sum(sales_price) , and 3.0 * count_price is an unbiased estimator for count(sales_price) . The dividing the sum by the count, we obtain the average. Note that those scaling factors differ depending on the source nodes (e.g., A1, C1, and C2). Step 3: plan simplification \u00b6 VerdictDB simplifies the plan if possible. This process helps avoiding unnecessary temporary table creations. Step 4: Execution / Cleaning \u00b6 VerdictDB executes the plan and removes the temporary tables if necessary. How Individual Aggregates Combined? \u00b6 VerdictDB applies different rules for different types of aggregate functions as follows. VerdictDB relies on the state-of-the-art techniques available in the literature. AVG, SUM, COUNT \u00b6 VerdictDB's answers are always unbiased estimators of the true answers. For instance, if only 10% of the data (that amounts to the 10% uniform random sample of the data) is processed, the unbiased estimator for the count function is 10 times the answer computed on the 10% of the data. This logic becomes more complex as unbiased samples (within scrambles) are used for different types of aggregate functions. VerdictDB performs these computations (and proper scaling) all automatically. MIN, MAX \u00b6 VerdictDB's answers to min and max functions are the min and max of the data that has been processed so far. For example, if 10% of the data was processed, then VerdictDB outputs min or max among those 10% data. Of course, the answers become more accurate as more data is processed and become exact when 100% data is processed. One possible concern is that the answers based on partial data may not be very accurate especially when a tiny fraction (e.g., 0.1%) of the data has been processed. To overcome this, VerdictDB processes outliers first. As a result, even the answers at the early stages are highly accurate. COUNT-DISTINCT \u00b6 This is in preparation.","title":"Query Processing"},{"location":"how_it_works/query_processing/#query-processing","text":"On this page, we describe how VerdictDB could speed up query processing. Note that for query processing, VerdictDB internally creates directed acyclic graph (DAG) representations (as described on this page ) and use it for processing queries. The key to VerdictDB's faster query processing is how the DAG is constructed, which we describe below.","title":"Query Processing"},{"location":"how_it_works/query_processing/#dag-construction","text":"For description, we use the following example query: select product , avg ( sales_price ) as avg_price from ( select product , price * ( 1 - discount ) as sales_price from sales_table_scramble where order_date between date '2018-01-01' and date '2018-01-31' ) t group by product order by avg_price desc In the above example, the inner query (projection) computes the price after discount, i.e., sales_price , and then the outer query (aggregation) computes the average of sales_price . Although this example query may be flattened into a simpler form, we intentionally use this nested form to make our description more general. Also, although VerdictDB internally parses the query (in String format) into its internal Java objects, our description will keep using the query string for easier understanding. We suppose that a scramble sales_table_scramble has been created for the sales_table table, and sales_table_scramble contains three blocks. As described on this page , each block of the scramble amounts to a random sample of the original table, i.e., sales_table .","title":"DAG Construction"},{"location":"how_it_works/query_processing/#step-1-regular-dag-construction","text":"The given query is decomposed into multiple queries, each of which is a flat query. Except for the root node, all other nodes include create table as select ... queries. The query for a parent node depends on its children. Below we depict the DAG and the queries for those nodes. -- Q1 select * from temp_table2 -- Q2 create table temp_table2 select product , avg ( sales_price ) as avg_price from temp_table1 t group by product order by avg_price desc -- Q3 create table temp_table1 select product , price * ( 1 - discount ) as sales_price from sales_table_scramble where order_date between date '2018-01-01' and date '2018-01-31'","title":"Step 1: regular DAG construction"},{"location":"how_it_works/query_processing/#step-2-progressive-aggregation-dag-construction","text":"VerdictDB converts a part of the DAG to enable progressive aggregations. The affected parts are the aggregate queries including scrambles in its from clause or the projections of scrambles. After the conversion, the DAG looks like below. First, each of the projection nodes at the bottom only involves a part of the scramble. In this simple example, the single scramble is split into three projections. See that the following query includes an extra filtering predicate, i.e., verdictdbblock = 0 , to only select the particular block. -- P1 create table temp_table1 select product , price * ( 1 - discount ) as sales_price from sales_table_scramble where order_date between date '2018-01-01' and date '2018-01-31' and verdictdbblock = 0 Second, the aggregation is separately computed for each of those projections. It is important to note that the original avg function was converted into two separate aggregate functions, i.e., sum and count . The avg function value will be restored later. -- A1 create table temp_table2 select product , sum ( sales_price ) as sum_price , count ( sales_price ) as count_price from temp_table1 t group by product ; Observe that the individual aggregation nodes (A1, A2, and A3) only involves its own verdictdbblock, i.e., identified with 0, 1, and 2. To compute the exact answers, we combine those individual aggregates using additional nodes Combiners (C1 and C2). Naturally, the number of the Combiners is always one fewer than the number of individual aggregate nodes. Suppose A1 creates a temporary table temp_table2 , A2 creates temp_table3 , and A3 creates temp_table4 . Then, the Combiners perform the operations as follows. -- C1 create table temp_table5 select product , sum ( sum_price ) as sum_price , count ( count_price ) as count_price from ( select * from temp_table2 union all select * from temp_table3 ) t group by product ; The nodes also propagate some necessary metadata about the processed verdictdbblocks thus far. Finally, the node S collects those aggregates, scale them appropriately, and restore the original select items. -- S create table temp_table7 select product , ( 3 . 0 * sum_price ) / ( 3 . 0 * count_price ) as avg_price from temp_table5 group by product ; In the above query (for the node S), 3.0 * sum_price is an unbiased estimator for sum(sales_price) , and 3.0 * count_price is an unbiased estimator for count(sales_price) . The dividing the sum by the count, we obtain the average. Note that those scaling factors differ depending on the source nodes (e.g., A1, C1, and C2).","title":"Step 2: progressive aggregation DAG construction"},{"location":"how_it_works/query_processing/#step-3-plan-simplification","text":"VerdictDB simplifies the plan if possible. This process helps avoiding unnecessary temporary table creations.","title":"Step 3: plan simplification"},{"location":"how_it_works/query_processing/#step-4-execution-cleaning","text":"VerdictDB executes the plan and removes the temporary tables if necessary.","title":"Step 4: Execution / Cleaning"},{"location":"how_it_works/query_processing/#how-individual-aggregates-combined","text":"VerdictDB applies different rules for different types of aggregate functions as follows. VerdictDB relies on the state-of-the-art techniques available in the literature.","title":"How Individual Aggregates Combined?"},{"location":"how_it_works/query_processing/#avg-sum-count","text":"VerdictDB's answers are always unbiased estimators of the true answers. For instance, if only 10% of the data (that amounts to the 10% uniform random sample of the data) is processed, the unbiased estimator for the count function is 10 times the answer computed on the 10% of the data. This logic becomes more complex as unbiased samples (within scrambles) are used for different types of aggregate functions. VerdictDB performs these computations (and proper scaling) all automatically.","title":"AVG, SUM, COUNT"},{"location":"how_it_works/query_processing/#min-max","text":"VerdictDB's answers to min and max functions are the min and max of the data that has been processed so far. For example, if 10% of the data was processed, then VerdictDB outputs min or max among those 10% data. Of course, the answers become more accurate as more data is processed and become exact when 100% data is processed. One possible concern is that the answers based on partial data may not be very accurate especially when a tiny fraction (e.g., 0.1%) of the data has been processed. To overcome this, VerdictDB processes outliers first. As a result, even the answers at the early stages are highly accurate.","title":"MIN, MAX"},{"location":"how_it_works/query_processing/#count-distinct","text":"This is in preparation.","title":"COUNT-DISTINCT"},{"location":"reference/append_scrambling/","text":"Appending Scrambles \u00b6 You can append new data into existing scrambles. Syntax for Appending Data to an Existing Scramble \u00b6 [ INSERT | APPEND ] SCRAMBLE existingSchema . existingTable WHERE condition Note: VerdictDB obtains new data from the original table used to create the scramble existingSchema.existingTable that satisfty given condition (e.g., year=2019). The same method (e.g., uniform or hash) used to create the existing scramble will be used to append new data. It is user's responsibility to make sure that (s)he does not append duplicate data into existing schema. Example of Appending Data to an Existing Scramble \u00b6 For example, if you want to add new data collected in the year of 2019 into existing scramble ourSchema.scrmableTable (assuming the scramble only contains data up to 2018), you may issue the following query to VerdictDB: APPEND SCRAMBLE ourSchema . scrambleTable WHERE year = 2019 ;","title":"Appending Scrambles"},{"location":"reference/append_scrambling/#appending-scrambles","text":"You can append new data into existing scrambles.","title":"Appending Scrambles"},{"location":"reference/append_scrambling/#syntax-for-appending-data-to-an-existing-scramble","text":"[ INSERT | APPEND ] SCRAMBLE existingSchema . existingTable WHERE condition Note: VerdictDB obtains new data from the original table used to create the scramble existingSchema.existingTable that satisfty given condition (e.g., year=2019). The same method (e.g., uniform or hash) used to create the existing scramble will be used to append new data. It is user's responsibility to make sure that (s)he does not append duplicate data into existing schema.","title":"Syntax for Appending Data to an Existing Scramble"},{"location":"reference/append_scrambling/#example-of-appending-data-to-an-existing-scramble","text":"For example, if you want to add new data collected in the year of 2019 into existing scramble ourSchema.scrmableTable (assuming the scramble only contains data up to 2018), you may issue the following query to VerdictDB: APPEND SCRAMBLE ourSchema . scrambleTable WHERE year = 2019 ;","title":"Example of Appending Data to an Existing Scramble"},{"location":"reference/connection/","text":"Connecting to Databases \u00b6 VerdictDB can process the data stored in a SQL-supported database; therefore, no data migration is required outside your database. VerdictDB works with them by performing its operations in SQL. Supported Databases \u00b6 MySQL 5.5 or later PostgreSQL 10 or later Amazon Redshift Impala 2.5 (CDH 5.7) or later Presto (Hive-Connector) Spark 2.0 or later Making a Connection in Java \u00b6 VerdictDB offers the standard JDBC interface. If the verdict keyword appears after jdbc: , Java uses VerdictDB to connect a backend database. In general, they have the following pattern. Connection vc = DriverManager.getConnection(\"connection_string\", \"user\", \"password\"); MySQL: \"jdbc:verdict:mysql://localhost:8080/test\" PostgreSQL: \"jdbc:verdict:postgresql://localhost:5432/database\" Redshift: \"jdbc:verdict:redshift://host:5439/dev\" Impala: \"jdbc:verdict:impala://localhost:21050/default\" Presto: \"jdbc:verdict:presto://localhost:8080/default\" For more details on configuration options, see this page . Apache Spark \u00b6 (Documentation not ready)","title":"Connecting to Databases"},{"location":"reference/connection/#connecting-to-databases","text":"VerdictDB can process the data stored in a SQL-supported database; therefore, no data migration is required outside your database. VerdictDB works with them by performing its operations in SQL.","title":"Connecting to Databases"},{"location":"reference/connection/#supported-databases","text":"MySQL 5.5 or later PostgreSQL 10 or later Amazon Redshift Impala 2.5 (CDH 5.7) or later Presto (Hive-Connector) Spark 2.0 or later","title":"Supported Databases"},{"location":"reference/connection/#making-a-connection-in-java","text":"VerdictDB offers the standard JDBC interface. If the verdict keyword appears after jdbc: , Java uses VerdictDB to connect a backend database. In general, they have the following pattern. Connection vc = DriverManager.getConnection(\"connection_string\", \"user\", \"password\"); MySQL: \"jdbc:verdict:mysql://localhost:8080/test\" PostgreSQL: \"jdbc:verdict:postgresql://localhost:5432/database\" Redshift: \"jdbc:verdict:redshift://host:5439/dev\" Impala: \"jdbc:verdict:impala://localhost:21050/default\" Presto: \"jdbc:verdict:presto://localhost:8080/default\" For more details on configuration options, see this page .","title":"Making a Connection in Java"},{"location":"reference/connection/#apache-spark","text":"(Documentation not ready)","title":"Apache Spark"},{"location":"reference/drop_scrambling/","text":"Dropping Scrambles \u00b6 You can either drop 1) a specific scramble; or 2) all scrambles that belong to a single table. Syntax for Dropping a Specific Scramble \u00b6 DROP SCRAMBLE scrambleSchema . scrambleTable ON originalSchema . originalTable ; This will remove the scrambleSchema.scrambleTable scramble built for originalSchema.originalTable and its metadata in VerdictDB. Syntax for Dropping All Scrambles for a Table \u00b6 DROP ALL SCRAMBLE originalSchema . originalTable ; This will remove all scrambles built for originalSchema.originalTable and their metadata in VerdictDB.","title":"Dropping Scrambles"},{"location":"reference/drop_scrambling/#dropping-scrambles","text":"You can either drop 1) a specific scramble; or 2) all scrambles that belong to a single table.","title":"Dropping Scrambles"},{"location":"reference/drop_scrambling/#syntax-for-dropping-a-specific-scramble","text":"DROP SCRAMBLE scrambleSchema . scrambleTable ON originalSchema . originalTable ; This will remove the scrambleSchema.scrambleTable scramble built for originalSchema.originalTable and its metadata in VerdictDB.","title":"Syntax for Dropping a Specific Scramble"},{"location":"reference/drop_scrambling/#syntax-for-dropping-all-scrambles-for-a-table","text":"DROP ALL SCRAMBLE originalSchema . originalTable ; This will remove all scrambles built for originalSchema.originalTable and their metadata in VerdictDB.","title":"Syntax for Dropping All Scrambles for a Table"},{"location":"reference/javadoc/","text":"Javadoc \u00b6 You can view Javadoc for VerdictDB here .","title":"Javadoc"},{"location":"reference/javadoc/#javadoc","text":"You can view Javadoc for VerdictDB here .","title":"Javadoc"},{"location":"reference/properties/","text":"VerdictDB JDBC Properties \u00b6 VerdictDB supports a number of configurable properties, which can be written as key=value pairs inside JDBC connection string. The currently supported properties are as follows: verdictdbmetaschema : sets the name of schema/database that VerdictDB will use to store metadata of scrambled tables. verdictdbtempschema : sets the name of schema/database that VerdictDB will use to create scrambled tables. loglevel : sets the minimum level of logs that VerdictDB will print out to the console. Possible values are {error, warn, info, debug, trace}. file_loglevel : sets the minimum level of logs that VerdictDB will print out to the log file . Possible values are {error, warn, info, debug, trace}. For example, a JDBC connection string with VerdictDB-specific properties can be written as: String connectionString = String . format ( \"jdbc:verdict:mysql://%s:%d/%s?verdictdbmetaschema=myverdictdbmeta&\" + \"verdictdbtempschema=myverdictdbtemp&loglevel=debug\" , MYSQL_HOST , MYSQL_PORT , MYSQL_DATABASE ); Connection vc = DriverManager . getConnection ( connectionString , MYSQL_USER , MYSQL_PASSWORD ); With the above JDBC connection string, VerdictDB will create metadata tables in the database myverdictdbmeta and scrambled tables in myverdictdbtemp , and every log with log level higher or equals to debug will be printed out to the console.","title":"VerdictDB JDBC Properties"},{"location":"reference/properties/#verdictdb-jdbc-properties","text":"VerdictDB supports a number of configurable properties, which can be written as key=value pairs inside JDBC connection string. The currently supported properties are as follows: verdictdbmetaschema : sets the name of schema/database that VerdictDB will use to store metadata of scrambled tables. verdictdbtempschema : sets the name of schema/database that VerdictDB will use to create scrambled tables. loglevel : sets the minimum level of logs that VerdictDB will print out to the console. Possible values are {error, warn, info, debug, trace}. file_loglevel : sets the minimum level of logs that VerdictDB will print out to the log file . Possible values are {error, warn, info, debug, trace}. For example, a JDBC connection string with VerdictDB-specific properties can be written as: String connectionString = String . format ( \"jdbc:verdict:mysql://%s:%d/%s?verdictdbmetaschema=myverdictdbmeta&\" + \"verdictdbtempschema=myverdictdbtemp&loglevel=debug\" , MYSQL_HOST , MYSQL_PORT , MYSQL_DATABASE ); Connection vc = DriverManager . getConnection ( connectionString , MYSQL_USER , MYSQL_PASSWORD ); With the above JDBC connection string, VerdictDB will create metadata tables in the database myverdictdbmeta and scrambled tables in myverdictdbtemp , and every log with log level higher or equals to debug will be printed out to the console.","title":"VerdictDB JDBC Properties"},{"location":"reference/query_syntax/","text":"VerdictDB can give interactive answers for aggregate queries, which include avg , sum , count , min , and max . The SQL query syntax is almost identical to the standard. SELECT [ select_item , ] aggregate_expr [, aggregate_expr ] [, ...] FROM table_source [ WHERE predicate ] [ GROUP BY group_expr ] [ ORDER BY ordering_expr ] [ LIMIT number ]; select_item : = regular_expr [ AS alias ]; table_source : = base_table | base_table , table_source | base_table [ INNER | LEFT | RIGHT ] JOIN table_source ON condition | select_query ; group_expr : = regular_expr | alias ; aggregate_expr : = avg ( regular_expr ) | sum ( regular_expr ) | count ( regular_expr ) | count ( distinct regular_expr ) | min ( regular_expr ) | max ( regular_expr ); regular_expr : = unary_func ( regular_expr ) | binary_func ( regular_expr , regular_expr ) | regular_expr op regular_expr ; op : = + | - | * | / | and | or ; Note \u00b6 Names and aliases can be quoted either using double quotes or backticks. Almost all functions can be used (for \"unary_func\", \"binary_func\") if those functions are supported by the backend database.","title":"Select-Query Syntax"},{"location":"reference/query_syntax/#note","text":"Names and aliases can be quoted either using double quotes or backticks. Almost all functions can be used (for \"unary_func\", \"binary_func\") if those functions are supported by the backend database.","title":"Note"},{"location":"reference/querying/","text":"Simple Querying \u00b6 VerdictDB's basic query interface returns a single (approximate) result set (i.e., a table) given a query. This is the same interface as what most databases offer; thus, this interface is convenient to use VerdictDB as a drop-in-replacement of other databases. There are two approaches to VerdictDB's basic query interface: VerdicDB JDBC Driver's regular executeQuery() method VerdictContext's sql() method We describe them in more detail below. VerdictDB's JDBC Driver \u00b6 Suppose we aim to query the average price of the items in the sales table, i.e., select avg(price) from sales . Then, issuing the query with sales_scramble table in a traditional way to the VerdictDB's JDBC interface returns an approximate answer. The below code shows an example. Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement stmt = verdict . createStatement (); ResultSet rs = stmt . executeQuery ( \"select avg(price) from sales_scramble\" ); Note: Direct querying to the backend You can always send a query directly to the backend database by preceding a query with the bypass keyword, e.g., bypass select avg(price) from sales . Then, VerdictDB sends the query to the backend database without any query rewriting or extra processing. This approach can be used for all other queries, such as create schema ... , set key=value , etc. VerdictContext \u00b6 Issuing a query using VerdictContext.sql() method returns a single approximate query result. See the example below. String connectionString = \"jdbc:mysql://localhost?user=root&password=rootpassword\" ; VerdictContext verdict = VerdictContext . fromJdbcConnectionString ( connectionString ); VerdictSingleResult rs = verdict . sql ( \"select avg(price) from sales_scramble\" ); Note When creating an instance of VerdictContext, the JDBC url must not include the \"verdict\" keyword.","title":"Simple Querying"},{"location":"reference/querying/#simple-querying","text":"VerdictDB's basic query interface returns a single (approximate) result set (i.e., a table) given a query. This is the same interface as what most databases offer; thus, this interface is convenient to use VerdictDB as a drop-in-replacement of other databases. There are two approaches to VerdictDB's basic query interface: VerdicDB JDBC Driver's regular executeQuery() method VerdictContext's sql() method We describe them in more detail below.","title":"Simple Querying"},{"location":"reference/querying/#verdictdbs-jdbc-driver","text":"Suppose we aim to query the average price of the items in the sales table, i.e., select avg(price) from sales . Then, issuing the query with sales_scramble table in a traditional way to the VerdictDB's JDBC interface returns an approximate answer. The below code shows an example. Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement stmt = verdict . createStatement (); ResultSet rs = stmt . executeQuery ( \"select avg(price) from sales_scramble\" ); Note: Direct querying to the backend You can always send a query directly to the backend database by preceding a query with the bypass keyword, e.g., bypass select avg(price) from sales . Then, VerdictDB sends the query to the backend database without any query rewriting or extra processing. This approach can be used for all other queries, such as create schema ... , set key=value , etc.","title":"VerdictDB's JDBC Driver"},{"location":"reference/querying/#verdictcontext","text":"Issuing a query using VerdictContext.sql() method returns a single approximate query result. See the example below. String connectionString = \"jdbc:mysql://localhost?user=root&password=rootpassword\" ; VerdictContext verdict = VerdictContext . fromJdbcConnectionString ( connectionString ); VerdictSingleResult rs = verdict . sql ( \"select avg(price) from sales_scramble\" ); Note When creating an instance of VerdictContext, the JDBC url must not include the \"verdict\" keyword.","title":"VerdictContext"},{"location":"reference/scrambling/","text":"Creating/Viewing Scrambles \u00b6 A scramble is a special table used by VerdictDB to speed up query processing. The information about the created scrambles is stored in its metadata table and is used at query time. Syntax for Creating Scrambles \u00b6 CREATE SCRAMBLE newSchema . newTable FROM originalSchema . originalTable [ METHOD ( uniform | hash )] [( ON | HASHCOLUMN ) hashcolumn ] [( SIZE | RATIO ) sizeOfScramble ] Note: newSchema may be identical to originalSchema . newTable must be different from originalTable if newSchema is same as originalSchema . The user requires the write privilege for the newSchema schema and the read privilege for the originalSchema.originalTable . METHOD must be either \"uniform\" or \"hash\". A uniform scramble is used for count, sum, avg, max, and min. A hash scramble is used for count-distinct. If a hash scramble is to be built, the hashcolumn must be present. hashcolumn indicates the column that will appear within the count-distinct function (e.g., count(distinct hashcolumn) ). sizeOfScramble (default = 1.0) defines the relative size of the scramble to its original table and must be a float value between 0.0 and 1.0 (e.g., sizeOfScramble=0.1 will create a scramble which size is 10% of the original table). The schema and table names can be quoted either using the double-quote (\") or the backtick (`). Syntax for Viewing Scrambles \u00b6 SHOW SCRAMBLES ; This query will print the list of all scrambled tables that have been built.","title":"Creating/Viewing Scrambles"},{"location":"reference/scrambling/#creatingviewing-scrambles","text":"A scramble is a special table used by VerdictDB to speed up query processing. The information about the created scrambles is stored in its metadata table and is used at query time.","title":"Creating/Viewing Scrambles"},{"location":"reference/scrambling/#syntax-for-creating-scrambles","text":"CREATE SCRAMBLE newSchema . newTable FROM originalSchema . originalTable [ METHOD ( uniform | hash )] [( ON | HASHCOLUMN ) hashcolumn ] [( SIZE | RATIO ) sizeOfScramble ] Note: newSchema may be identical to originalSchema . newTable must be different from originalTable if newSchema is same as originalSchema . The user requires the write privilege for the newSchema schema and the read privilege for the originalSchema.originalTable . METHOD must be either \"uniform\" or \"hash\". A uniform scramble is used for count, sum, avg, max, and min. A hash scramble is used for count-distinct. If a hash scramble is to be built, the hashcolumn must be present. hashcolumn indicates the column that will appear within the count-distinct function (e.g., count(distinct hashcolumn) ). sizeOfScramble (default = 1.0) defines the relative size of the scramble to its original table and must be a float value between 0.0 and 1.0 (e.g., sizeOfScramble=0.1 will create a scramble which size is 10% of the original table). The schema and table names can be quoted either using the double-quote (\") or the backtick (`).","title":"Syntax for Creating Scrambles"},{"location":"reference/scrambling/#syntax-for-viewing-scrambles","text":"SHOW SCRAMBLES ; This query will print the list of all scrambled tables that have been built.","title":"Syntax for Viewing Scrambles"},{"location":"reference/scrambling_syntax/","text":"Scrambling Syntax \u00b6 CREATE SCRAMBLE newSchema . newTable FROM originalSchema . originalTable ; Note: newSchema may be identical to originalSchema . newTable must be different from originalTable . The user requires the write privilege for the newSchema schema and the read privilege for the originalSchema.originalTable . The schema and table names can be quoted either using the double-quote (\") or the backtick (`). The schema names, table names, column names, etc. in the queries issued by VerdictDB to the backend database are always quoted. What is a Scramble? \u00b6 For VerdictDB's interactive query processing, a special table called a scramble must be created for an original table. The queries including scrambles are automatically rewritten by VerdictDB in a way to enable interactive querying. The queries including the original table(s) are also first rewritten to use its/their corresponding scrambles; then, VerdictDB applies to the same mechanism to enable interactive querying for those queries. If no scrambles have been created for a table, no query rewritting is performed. Every time a scramble is created, VerdictDB stores the information in its own metadata schema ( verdictdbmetadata by default). The relationship between the original tables and scrambles are recognized using this metadata.","title":"Scrambling Syntax"},{"location":"reference/scrambling_syntax/#scrambling-syntax","text":"CREATE SCRAMBLE newSchema . newTable FROM originalSchema . originalTable ; Note: newSchema may be identical to originalSchema . newTable must be different from originalTable . The user requires the write privilege for the newSchema schema and the read privilege for the originalSchema.originalTable . The schema and table names can be quoted either using the double-quote (\") or the backtick (`). The schema names, table names, column names, etc. in the queries issued by VerdictDB to the backend database are always quoted.","title":"Scrambling Syntax"},{"location":"reference/scrambling_syntax/#what-is-a-scramble","text":"For VerdictDB's interactive query processing, a special table called a scramble must be created for an original table. The queries including scrambles are automatically rewritten by VerdictDB in a way to enable interactive querying. The queries including the original table(s) are also first rewritten to use its/their corresponding scrambles; then, VerdictDB applies to the same mechanism to enable interactive querying for those queries. If no scrambles have been created for a table, no query rewritting is performed. Every time a scramble is created, VerdictDB stores the information in its own metadata schema ( verdictdbmetadata by default). The relationship between the original tables and scrambles are recognized using this metadata.","title":"What is a Scramble?"},{"location":"reference/streaming/","text":"Stream Querying \u00b6 (This feature is experimental and less tested.) Stream querying is a feature of VerdictDB that allow users to retrieve results from the query while the query is still processing. VerdictDB uses scrambles to process the query. In scrambles, original tables are partitioned into many blocks. When processing query, VerdictDB estimates the query, starting from one block and gradually increasing the blocks. The process will finish until covering all blocks(In that case, the results returned by VerdictDB are exactly the same with directly querying from database). Every time the estimation of blocks is done, it will return the result to users. Users can decide whether to have fast but less accurate results or take time to obtain more accurate results. There are two approaches to VerdictDB's stream querying: VerdicDB JDBC Driver's regular executeQuery() method VerdictContext's streamsql() method We describe them in more detail below. Use Stream Querying in VerdictStatement \u00b6 Syntax \u00b6 STREAM [SELECT QUERY] Note: [SELECT QUERY] is the SQL query you want to process. Example \u00b6 Assume we want to find the average l_extendedprice in lineitem table. Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement vstmt = verdict . createStatement (); ResultSet vrs = vstmt . executeQuery ( \"STREAM SELECT AVG(l_extendedprice) from lineitem\" ); Then we can get the approximate results from vrs in JDBC way. while ( vrs . next ()) { System . out . println ( vrs . getDouble ( 1 )); } Note: The behavior of Stream Querying Suppose the scramble of lineitem has 10 blocks from block0 to block9. VerdictDB will first obtain the result from block0, then block0+block1, then block0+block1+block2 and so on, until it covers all blocks. Once the estimation of block0 is done, it will return the result to vrs and start to process block0+block1. So, user will eventually get 10 results from this query in the order of increasing number of covering blocks. When user call next() and VerdictDB is still computing for the next result, user will get blocked until the next result is ready. Use Stream Querying in VerdictContext \u00b6 We can issue the query to streamsql() method to do stream query. Example \u00b6 String connectionString = \"jdbc:mysql://localhost?user=root&password=rootpassword\" ; VerdictContext verdict = VerdictContext . fromJdbcConnectionString ( connectionString ); VerdictResultStream vrs = verdict . streamsql ( \"SELECT AVG(l_extendedprice) from lineitem\" ); Note: The input SQL query of streamsql() does not have the prefix STREAM . VerdictResultStream type extends Iterable<VerdictSingleResult> , Iterator<VerdictSingleResult> Retrieve the results of stream querying. while (vrs.hasNext()){ VerdictSingleResult verdictSingleResult = vrs.next(); System.out.println(verdictSingleResult.getDouble(1)); } Additionally, you can use the verdictSingleResult.print() method to print the contents of VerdictSingleResult look like a table with rows and columns with borders. For more information about this method, you can check the Java utility class DBTablePrinter .","title":"Stream Querying"},{"location":"reference/streaming/#stream-querying","text":"(This feature is experimental and less tested.) Stream querying is a feature of VerdictDB that allow users to retrieve results from the query while the query is still processing. VerdictDB uses scrambles to process the query. In scrambles, original tables are partitioned into many blocks. When processing query, VerdictDB estimates the query, starting from one block and gradually increasing the blocks. The process will finish until covering all blocks(In that case, the results returned by VerdictDB are exactly the same with directly querying from database). Every time the estimation of blocks is done, it will return the result to users. Users can decide whether to have fast but less accurate results or take time to obtain more accurate results. There are two approaches to VerdictDB's stream querying: VerdicDB JDBC Driver's regular executeQuery() method VerdictContext's streamsql() method We describe them in more detail below.","title":"Stream Querying"},{"location":"reference/streaming/#use-stream-querying-in-verdictstatement","text":"","title":"Use Stream Querying in VerdictStatement"},{"location":"reference/streaming/#syntax","text":"STREAM [SELECT QUERY] Note: [SELECT QUERY] is the SQL query you want to process.","title":"Syntax"},{"location":"reference/streaming/#example","text":"Assume we want to find the average l_extendedprice in lineitem table. Connection verdict = DriverManager . getConnection ( \"jdbc:verdict:mysql://localhost\" , \"root\" , \"rootpassword\" ); Statement vstmt = verdict . createStatement (); ResultSet vrs = vstmt . executeQuery ( \"STREAM SELECT AVG(l_extendedprice) from lineitem\" ); Then we can get the approximate results from vrs in JDBC way. while ( vrs . next ()) { System . out . println ( vrs . getDouble ( 1 )); } Note: The behavior of Stream Querying Suppose the scramble of lineitem has 10 blocks from block0 to block9. VerdictDB will first obtain the result from block0, then block0+block1, then block0+block1+block2 and so on, until it covers all blocks. Once the estimation of block0 is done, it will return the result to vrs and start to process block0+block1. So, user will eventually get 10 results from this query in the order of increasing number of covering blocks. When user call next() and VerdictDB is still computing for the next result, user will get blocked until the next result is ready.","title":"Example"},{"location":"reference/streaming/#use-stream-querying-in-verdictcontext","text":"We can issue the query to streamsql() method to do stream query.","title":"Use Stream Querying in VerdictContext"},{"location":"reference/streaming/#example_1","text":"String connectionString = \"jdbc:mysql://localhost?user=root&password=rootpassword\" ; VerdictContext verdict = VerdictContext . fromJdbcConnectionString ( connectionString ); VerdictResultStream vrs = verdict . streamsql ( \"SELECT AVG(l_extendedprice) from lineitem\" ); Note: The input SQL query of streamsql() does not have the prefix STREAM . VerdictResultStream type extends Iterable<VerdictSingleResult> , Iterator<VerdictSingleResult> Retrieve the results of stream querying. while (vrs.hasNext()){ VerdictSingleResult verdictSingleResult = vrs.next(); System.out.println(verdictSingleResult.getDouble(1)); } Additionally, you can use the verdictSingleResult.print() method to print the contents of VerdictSingleResult look like a table with rows and columns with borders. For more information about this method, you can check the Java utility class DBTablePrinter .","title":"Example"},{"location":"reference/supported_queries/","text":"VerdictDB Queries \u00b6 Supported queries \u00b6 Overview \u00b6 Verdict brings significant speedups for many important analytic queries. Before providing a detailed presentation on the queries Verdict can bring speedups, the following summary provides a quick overview. Verdict brings speedups for the queries including aggregate functions . Verdict can speedup most common aggregate functions The only known exceptions are extreme statistics: min and max . Verdict can speedup the queries including joins of multiple tables and subqueries. For relatively simple queries (whose depth is no more than three), Verdict mostly brings speedups. For deeper, complex queries (such as aggregations over aggregations), speedups more depend on Verdict's sample planner. The cost of the worst cases will simply be equivalent to running the original queries. Querying database metadata \u00b6 show databases; Displays database names. When Verdict runs on Spark, it displays the tables accessible through HiveContext . use database_name; Set the default database to database-name . show tables [in database_name]; Displays the tables in database-name . If the database name is not specified, the tables in the default database are listed. If database-name is not specified and the default database is not chosen, Verdict throws an error. describe [database_name].table_name; Displays the column names and their types. show samples [for database_name]; Displays the samples created for the tables in the database-name . If database-name is not specified and the database name is not specified, the samples created for the default database are specified. If the default database is not chosen, Verdict throws an error. Creating samples \u00b6 create [XX%] sample of [database_name.]table_name; Creates a set of samples for the specified table. This is the recommended way of creating sample tables. Verdict analyzes the statistics of the table and automatically creates desired samples. If the sampling probability is omitted, 1% samples are created by default. Currently, Verdict creates three types of samples using the following rule: A uniform random sample Stratified samples for low-cardinality columns (distinct-count of a column <= 1% of the total number of tuples). Universe samples for high-cardinality columns (distinct-count of a column > 1% of the total number of tuples). Find more details on each sample type below. create [XX%] uniform sample of [database_name.]table_name; Creates XX% (1% by default) uniform random sample of the specified table. Uniform random samples are useful for estimating some basic statistics, such as the number of tuples in a table, average of some expressions, etc., especially when there are no group-by clause. create [XX%] stratified sample of [database_name.]table_name on column_name; Creates XX% (1% by default) stratified sample of the specified table. Stratified samples ensures that no attribute values are dropped for the column(s) on which the stratified samples are built on. This implies that, in the result set of select group-name, count(*) from t group by group-name , every group-name exists even if Verdict runs the query on a sample. Stratified samples can be very useful when users have some knowledge on what columns will appear frequently in the group-by clause or in the where clause. For example, stratified samples can be very useful for streaming data where each record contains a timestamp. Building a stratified sample on the timestamp will ensure rare events are never dropped in the sampling process. Note that the sampling probabilities for tuples are not uniform in stratified sampling. However, Verdict automatically considers them and produce correct answers. create [XX%] universe sample of [database_name.]table_name on column_name; Creates XX% (1% by default) universe sample of the specified table. Universe samples are hashing-based sampling. Verdict uses hash functions available in a database system it works with. Universe samples are used for estimating distinct-count of high-cardinality columns. The theory behind using universe samples for distinct-count is closely related to the HyperLogLog algorithm <https://en.wikipedia.org/wiki/HyperLogLog> _. Different from HyperLogLog, however, Verdict's approach uses a sample; thus, it is significantly faster than HyperLogLog or any similar approaches that scan the entire data. Also, universe samples are useful when a query includes joins. The equi-joins of two universe samples (of different tables) built on the join key preserves the cardinality very well; thus, it can produce accurate answers compared to joining two uniform or stratified samples. Analyzing data: query structure \u00b6 Verdict processes the standard SQL query in the following form:: select expr1, expr2, ... from table_source1, table_source2, ... [where conditions] [group by expr1, ...] [order by expr1] [limit n;] Find more details on the supported statements below. Analyzing data: aggregate functions \u00b6 Supported aggregate functions \u00b6 Verdict brings speedups for the following aggregate functions or combinations of them: Aggregate function Description count(*) Counts the number of tuples that satisfy selection conditions in the where clause (if any) sum(col-name) Computes the summation of the non-null attribute values in the \"col-name\" column. avg(col-name) Computes the avreage of the non-null attribute values in the \"col-name\" column. count(distinct col-name) Computes the number of distinct attributes in the \"col-name\" column; only one column can be specified. Future supported aggregate functions \u00b6 Verdict will be extended to support the following aggregate functions in the future: Aggregate function Description var_pop(col-name) population variance var_samp(col-name) sample variance stddev_pop(col-name) population standard deviation stddev_samp(col-name) sample standard deviation covar_pop(col1, col2) population covariance covar_samp(col1, col2) sample covariance corr(col1, col2) Pearson correlation coefficient percentile(col1, p) p should be within 0.01 and 0.99 for reliable results No-speedup aggregate functions \u00b6 Verdict does not bring speedups (even in the future) for the following extreme statistics: Aggregate function Description min(col-name) Min of the attribute values in the \"col-name\" column max(col-name) Max of the attribute values in the \"col-name\" column If a query includes these no-speedup aggregate function(s), Verdict uses the original tables (instead of the sample tables) for processing those queries. Analyzing data: other functions \u00b6 In general, every (non-aggregate) function that is provided by existing database systems can be processed by Verdict (since Verdict will simply pass those functions to those databases). Please inform us if you want certain functions to be included. We will quickly add them. Mathematical functions \u00b6 Function Description round(double a) - floor(double a) - ceil(double a) - exp(double a) - ln(double a) a natural logarithm log10(double a) log with base 10 log2(double a) log with base 2 sin(double a) - cos(double a) - tan(double a) - sign(double a) Returns the sign of a as '1.0' (if a is positive) or '-1.0' (if a is negative), '0.0' otherwise pmod(int a, int b) a mod b; supported for Hive and Spark; See this <page https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF> _ for more information. a % b a mod b rand(int seed) random number between 0 and 1 abs(double a), abs(int a) an absolute value sqrt(double a) - String operators \u00b6 Function Description conv(int num, int from_base, int to_base), conv(string num, int from_base, int to_base) Converts a number from a given base to another; supported for Hive and Spark substr(string a, int start, int len) Returns the portion of the string starting at a specified point with a specified maximum length. Other functions \u00b6 Function Description fnv_hash(expr) Returns a consistent 64-bit value derived from the input argument; supported for Impala; See this page <https://www.cloudera.com/documentation/enterprise/5-8-x/topics/impala_math_functions.html> _ for more information. md5(expr) Calculates an MD5 128-bit checksum for the string or binary; supported for Hive and Spark crc32(expr) Computes a cyclic redundancy check value for string or binary argument and returns bigint value; supported for Hive and Spark Analyzing data: table sources, filtering predicates, etc. \u00b6 Table sources \u00b6 You can use a single base table, equi-joined tables, or derived tables in the from clause. Verdict's sample planner automatically finds the best set of sample tables to process your queries. However, if samples must not be used for processing your queries (due to unguaranteed accuracy), Verdict will use the original tables. Verdict's sample planner is rather involved, so we will make a separate document for its description. Note : Verdict's query parser currently processes only inner joins, but it will be extended to process left outer and right outer joins. Filtering predicates (i.e., in the where clause) \u00b6 Predicate Description p1 AND p2 logical and of two predicates, p1 and p2 p1 OR p2 logical or of two predicates, p1 and p2 expr1 COMP expr2 comparison of two expressions, expr1 and expr2, using the comparison operator, COMP. The available comparison operators are =, >, <, <=, >=, <>, !=, !>, !<, <=, >=, <, >, !>, !< expr COMP (subquery) comparison of the value of expr and the value of subquery. The subquery must return a single row and a single column expr1 NOT? BETWEEN expr2 AND expr3 returns true if the value of expr1 is between the value of expr2 and the value of expr3. expr1 NOT? LIKE expr2 text pattern search using wild cards. See this page <https://www.w3schools.com/sql/sql_like.asp> _ for more information. expr IS NOT? NULL test if the value of the expression is null. Dropping samples \u00b6 (delete | drop) [XX%] samples of [database-name.]table-name; Drop all the samples created for the specified table. The sampling ratio is 1% is not specified explicitly. (delete | drop) [XX%] uniform samples of [database-name.]table-name; Drop the uniform random sample created for the specified table. The sampling ratio is 1% is not specified explicitly. (delete | drop) [XX%] stratified samples of [database-name.]table-name on column-name; Drop the stratified sample created for the specified table. The sampling ratio is 1% is not specified explicitly. (delete | drop) [XX%] universe samples of [database-name.]table-name on column-name; Drop the universe sample created for the specified table. The sampling ratio is 1% is not specified explicitly. (Later, Yongjoo will update the content)","title":"VerdictDB Queries"},{"location":"reference/supported_queries/#verdictdb-queries","text":"","title":"VerdictDB Queries"},{"location":"reference/supported_queries/#supported-queries","text":"","title":"Supported queries"},{"location":"reference/supported_queries/#overview","text":"Verdict brings significant speedups for many important analytic queries. Before providing a detailed presentation on the queries Verdict can bring speedups, the following summary provides a quick overview. Verdict brings speedups for the queries including aggregate functions . Verdict can speedup most common aggregate functions The only known exceptions are extreme statistics: min and max . Verdict can speedup the queries including joins of multiple tables and subqueries. For relatively simple queries (whose depth is no more than three), Verdict mostly brings speedups. For deeper, complex queries (such as aggregations over aggregations), speedups more depend on Verdict's sample planner. The cost of the worst cases will simply be equivalent to running the original queries.","title":"Overview"},{"location":"reference/supported_queries/#querying-database-metadata","text":"show databases; Displays database names. When Verdict runs on Spark, it displays the tables accessible through HiveContext . use database_name; Set the default database to database-name . show tables [in database_name]; Displays the tables in database-name . If the database name is not specified, the tables in the default database are listed. If database-name is not specified and the default database is not chosen, Verdict throws an error. describe [database_name].table_name; Displays the column names and their types. show samples [for database_name]; Displays the samples created for the tables in the database-name . If database-name is not specified and the database name is not specified, the samples created for the default database are specified. If the default database is not chosen, Verdict throws an error.","title":"Querying database metadata"},{"location":"reference/supported_queries/#creating-samples","text":"create [XX%] sample of [database_name.]table_name; Creates a set of samples for the specified table. This is the recommended way of creating sample tables. Verdict analyzes the statistics of the table and automatically creates desired samples. If the sampling probability is omitted, 1% samples are created by default. Currently, Verdict creates three types of samples using the following rule: A uniform random sample Stratified samples for low-cardinality columns (distinct-count of a column <= 1% of the total number of tuples). Universe samples for high-cardinality columns (distinct-count of a column > 1% of the total number of tuples). Find more details on each sample type below. create [XX%] uniform sample of [database_name.]table_name; Creates XX% (1% by default) uniform random sample of the specified table. Uniform random samples are useful for estimating some basic statistics, such as the number of tuples in a table, average of some expressions, etc., especially when there are no group-by clause. create [XX%] stratified sample of [database_name.]table_name on column_name; Creates XX% (1% by default) stratified sample of the specified table. Stratified samples ensures that no attribute values are dropped for the column(s) on which the stratified samples are built on. This implies that, in the result set of select group-name, count(*) from t group by group-name , every group-name exists even if Verdict runs the query on a sample. Stratified samples can be very useful when users have some knowledge on what columns will appear frequently in the group-by clause or in the where clause. For example, stratified samples can be very useful for streaming data where each record contains a timestamp. Building a stratified sample on the timestamp will ensure rare events are never dropped in the sampling process. Note that the sampling probabilities for tuples are not uniform in stratified sampling. However, Verdict automatically considers them and produce correct answers. create [XX%] universe sample of [database_name.]table_name on column_name; Creates XX% (1% by default) universe sample of the specified table. Universe samples are hashing-based sampling. Verdict uses hash functions available in a database system it works with. Universe samples are used for estimating distinct-count of high-cardinality columns. The theory behind using universe samples for distinct-count is closely related to the HyperLogLog algorithm <https://en.wikipedia.org/wiki/HyperLogLog> _. Different from HyperLogLog, however, Verdict's approach uses a sample; thus, it is significantly faster than HyperLogLog or any similar approaches that scan the entire data. Also, universe samples are useful when a query includes joins. The equi-joins of two universe samples (of different tables) built on the join key preserves the cardinality very well; thus, it can produce accurate answers compared to joining two uniform or stratified samples.","title":"Creating samples"},{"location":"reference/supported_queries/#analyzing-data-query-structure","text":"Verdict processes the standard SQL query in the following form:: select expr1, expr2, ... from table_source1, table_source2, ... [where conditions] [group by expr1, ...] [order by expr1] [limit n;] Find more details on the supported statements below.","title":"Analyzing data: query structure"},{"location":"reference/supported_queries/#analyzing-data-aggregate-functions","text":"","title":"Analyzing data: aggregate functions"},{"location":"reference/supported_queries/#supported-aggregate-functions","text":"Verdict brings speedups for the following aggregate functions or combinations of them: Aggregate function Description count(*) Counts the number of tuples that satisfy selection conditions in the where clause (if any) sum(col-name) Computes the summation of the non-null attribute values in the \"col-name\" column. avg(col-name) Computes the avreage of the non-null attribute values in the \"col-name\" column. count(distinct col-name) Computes the number of distinct attributes in the \"col-name\" column; only one column can be specified.","title":"Supported aggregate functions"},{"location":"reference/supported_queries/#future-supported-aggregate-functions","text":"Verdict will be extended to support the following aggregate functions in the future: Aggregate function Description var_pop(col-name) population variance var_samp(col-name) sample variance stddev_pop(col-name) population standard deviation stddev_samp(col-name) sample standard deviation covar_pop(col1, col2) population covariance covar_samp(col1, col2) sample covariance corr(col1, col2) Pearson correlation coefficient percentile(col1, p) p should be within 0.01 and 0.99 for reliable results","title":"Future supported aggregate functions"},{"location":"reference/supported_queries/#no-speedup-aggregate-functions","text":"Verdict does not bring speedups (even in the future) for the following extreme statistics: Aggregate function Description min(col-name) Min of the attribute values in the \"col-name\" column max(col-name) Max of the attribute values in the \"col-name\" column If a query includes these no-speedup aggregate function(s), Verdict uses the original tables (instead of the sample tables) for processing those queries.","title":"No-speedup aggregate functions"},{"location":"reference/supported_queries/#analyzing-data-other-functions","text":"In general, every (non-aggregate) function that is provided by existing database systems can be processed by Verdict (since Verdict will simply pass those functions to those databases). Please inform us if you want certain functions to be included. We will quickly add them.","title":"Analyzing data: other functions"},{"location":"reference/supported_queries/#mathematical-functions","text":"Function Description round(double a) - floor(double a) - ceil(double a) - exp(double a) - ln(double a) a natural logarithm log10(double a) log with base 10 log2(double a) log with base 2 sin(double a) - cos(double a) - tan(double a) - sign(double a) Returns the sign of a as '1.0' (if a is positive) or '-1.0' (if a is negative), '0.0' otherwise pmod(int a, int b) a mod b; supported for Hive and Spark; See this <page https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF> _ for more information. a % b a mod b rand(int seed) random number between 0 and 1 abs(double a), abs(int a) an absolute value sqrt(double a) -","title":"Mathematical functions"},{"location":"reference/supported_queries/#string-operators","text":"Function Description conv(int num, int from_base, int to_base), conv(string num, int from_base, int to_base) Converts a number from a given base to another; supported for Hive and Spark substr(string a, int start, int len) Returns the portion of the string starting at a specified point with a specified maximum length.","title":"String operators"},{"location":"reference/supported_queries/#other-functions","text":"Function Description fnv_hash(expr) Returns a consistent 64-bit value derived from the input argument; supported for Impala; See this page <https://www.cloudera.com/documentation/enterprise/5-8-x/topics/impala_math_functions.html> _ for more information. md5(expr) Calculates an MD5 128-bit checksum for the string or binary; supported for Hive and Spark crc32(expr) Computes a cyclic redundancy check value for string or binary argument and returns bigint value; supported for Hive and Spark","title":"Other functions"},{"location":"reference/supported_queries/#analyzing-data-table-sources-filtering-predicates-etc","text":"","title":"Analyzing data: table sources, filtering predicates, etc."},{"location":"reference/supported_queries/#table-sources","text":"You can use a single base table, equi-joined tables, or derived tables in the from clause. Verdict's sample planner automatically finds the best set of sample tables to process your queries. However, if samples must not be used for processing your queries (due to unguaranteed accuracy), Verdict will use the original tables. Verdict's sample planner is rather involved, so we will make a separate document for its description. Note : Verdict's query parser currently processes only inner joins, but it will be extended to process left outer and right outer joins.","title":"Table sources"},{"location":"reference/supported_queries/#filtering-predicates-ie-in-the-where-clause","text":"Predicate Description p1 AND p2 logical and of two predicates, p1 and p2 p1 OR p2 logical or of two predicates, p1 and p2 expr1 COMP expr2 comparison of two expressions, expr1 and expr2, using the comparison operator, COMP. The available comparison operators are =, >, <, <=, >=, <>, !=, !>, !<, <=, >=, <, >, !>, !< expr COMP (subquery) comparison of the value of expr and the value of subquery. The subquery must return a single row and a single column expr1 NOT? BETWEEN expr2 AND expr3 returns true if the value of expr1 is between the value of expr2 and the value of expr3. expr1 NOT? LIKE expr2 text pattern search using wild cards. See this page <https://www.w3schools.com/sql/sql_like.asp> _ for more information. expr IS NOT? NULL test if the value of the expression is null.","title":"Filtering predicates (i.e., in the where clause)"},{"location":"reference/supported_queries/#dropping-samples","text":"(delete | drop) [XX%] samples of [database-name.]table-name; Drop all the samples created for the specified table. The sampling ratio is 1% is not specified explicitly. (delete | drop) [XX%] uniform samples of [database-name.]table-name; Drop the uniform random sample created for the specified table. The sampling ratio is 1% is not specified explicitly. (delete | drop) [XX%] stratified samples of [database-name.]table-name on column-name; Drop the stratified sample created for the specified table. The sampling ratio is 1% is not specified explicitly. (delete | drop) [XX%] universe samples of [database-name.]table-name on column-name; Drop the universe sample created for the specified table. The sampling ratio is 1% is not specified explicitly. (Later, Yongjoo will update the content)","title":"Dropping samples"},{"location":"reference/what_is_scramble/","text":"What is a Scramble? \u00b6 For VerdictDB's interactive query processing, a special table called a scramble must be created for an original table. The queries including scrambles are automatically rewritten by VerdictDB in a way to enable interactive querying. The queries including the original table(s) are also first rewritten to use its/their corresponding scrambles; then, VerdictDB applies to the same mechanism to enable interactive querying for those queries. If no scrambles have been created for a table, no query rewritting is performed. Every time a scramble is created, VerdictDB stores the information in its own metadata schema ( verdictdbmetadata by default). The relationship between the original tables and scrambles are recognized using this metadata.","title":"What is a Scramble?"},{"location":"reference/what_is_scramble/#what-is-a-scramble","text":"For VerdictDB's interactive query processing, a special table called a scramble must be created for an original table. The queries including scrambles are automatically rewritten by VerdictDB in a way to enable interactive querying. The queries including the original table(s) are also first rewritten to use its/their corresponding scrambles; then, VerdictDB applies to the same mechanism to enable interactive querying for those queries. If no scrambles have been created for a table, no query rewritting is performed. Every time a scramble is created, VerdictDB stores the information in its own metadata schema ( verdictdbmetadata by default). The relationship between the original tables and scrambles are recognized using this metadata.","title":"What is a Scramble?"},{"location":"tutorial/advanced/","text":"ML-based Analytics \u00b6 Since the inner-loop of many ML-based data analytics techniques are essentially aggregations, those ML-based techniques can also be performed faster with VerdictDB. (We will add more details in the future) Decision Tree \u00b6 Logistic Regression \u00b6","title":"ML-based Analytics"},{"location":"tutorial/advanced/#ml-based-analytics","text":"Since the inner-loop of many ML-based data analytics techniques are essentially aggregations, those ML-based techniques can also be performed faster with VerdictDB. (We will add more details in the future)","title":"ML-based Analytics"},{"location":"tutorial/advanced/#decision-tree","text":"","title":"Decision Tree"},{"location":"tutorial/advanced/#logistic-regression","text":"","title":"Logistic Regression"},{"location":"tutorial/tpch/","text":"TPC-H Data Setup \u00b6 This is a step-by-step guide for setting up TPC-H data in different databases. This guide will use 1GB data. This guide assumes you have basic knowledge about issuing commands in a terminal application. Download Data \u00b6 Go to your work directory (say /home/username/workspace ) and download the data: $ cd /home/username/workspace $ curl http://dbgroup-internal.eecs.umich.edu/projects/verdictdb/tpch1g.zip -o tpch1g.zip Unzip the downloaded file: $ unzip tpch1g.zip It will create a new directory named tpch1g under your work directory. The directory contains 8 sub-directories for each of 8 tables. MySQL \u00b6 Create tables \u00b6 Connect to your MySQL database. Make sure you have already added MySQL to your PATH. $ mysql --local-infile -h 127 .0.0.1 -uroot Create a schema for test. (In mysql shell) > create database tpch1g ; > use tpch1g ; Create empty tables; simply copy and paste the following table definition statements into the MySQL shell. We will import the data later into these tables. -- nation CREATE TABLE IF NOT EXISTS tpch1g . nation ( ` n_nationkey ` INT , ` n_name ` CHAR ( 25 ), ` n_regionkey ` INT , ` n_comment ` VARCHAR ( 152 ), ` n_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` n_nationkey ` )); -- region CREATE TABLE IF NOT EXISTS tpch1g . region ( ` r_regionkey ` INT , ` r_name ` CHAR ( 25 ), ` r_comment ` VARCHAR ( 152 ), ` r_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` r_regionkey ` )); -- supplier CREATE TABLE IF NOT EXISTS tpch1g . supplier ( ` s_suppkey ` INT , ` s_name ` CHAR ( 25 ), ` s_address ` VARCHAR ( 40 ), ` s_nationkey ` INT , ` s_phone ` CHAR ( 15 ), ` s_acctbal ` DECIMAL ( 15 , 2 ), ` s_comment ` VARCHAR ( 101 ), ` s_dummy ` varchar ( 10 ), PRIMARY KEY ( ` s_suppkey ` )); -- customer CREATE TABLE IF NOT EXISTS tpch1g . customer ( ` c_custkey ` INT , ` c_name ` VARCHAR ( 25 ), ` c_address ` VARCHAR ( 40 ), ` c_nationkey ` INT , ` c_phone ` CHAR ( 15 ), ` c_acctbal ` DECIMAL ( 15 , 2 ), ` c_mktsegment ` CHAR ( 10 ), ` c_comment ` VARCHAR ( 117 ), ` c_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` c_custkey ` )); -- part CREATE TABLE IF NOT EXISTS tpch1g . part ( ` p_partkey ` INT , ` p_name ` VARCHAR ( 55 ), ` p_mfgr ` CHAR ( 25 ), ` p_brand ` CHAR ( 10 ), ` p_type ` VARCHAR ( 25 ), ` p_size ` INT , ` p_container ` CHAR ( 10 ), ` p_retailprice ` DECIMAL ( 15 , 2 ) , ` p_comment ` VARCHAR ( 23 ) , ` p_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` p_partkey ` )); -- partsupp CREATE TABLE IF NOT EXISTS tpch1g . partsupp ( ` ps_partkey ` INT , ` ps_suppkey ` INT , ` ps_availqty ` INT , ` ps_supplycost ` DECIMAL ( 15 , 2 ), ` ps_comment ` VARCHAR ( 199 ), ` ps_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` ps_partkey ` )); -- orders CREATE TABLE IF NOT EXISTS tpch1g . orders ( ` o_orderkey ` INT , ` o_custkey ` INT , ` o_orderstatus ` CHAR ( 1 ), ` o_totalprice ` DECIMAL ( 15 , 2 ), ` o_orderdate ` DATE , ` o_orderpriority ` CHAR ( 15 ), ` o_clerk ` CHAR ( 15 ), ` o_shippriority ` INT , ` o_comment ` VARCHAR ( 79 ), ` o_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` o_orderkey ` )); -- lineitem CREATE TABLE IF NOT EXISTS tpch1g . lineitem ( ` l_orderkey ` INT , ` l_partkey ` INT , ` l_suppkey ` INT , ` l_linenumber ` INT , ` l_quantity ` DECIMAL ( 15 , 2 ), ` l_extendedprice ` DECIMAL ( 15 , 2 ), ` l_discount ` DECIMAL ( 15 , 2 ), ` l_tax ` DECIMAL ( 15 , 2 ), ` l_returnflag ` CHAR ( 1 ), ` l_linestatus ` CHAR ( 1 ), ` l_shipdate ` DATE , ` l_commitdate ` DATE , ` l_receiptdate ` DATE , ` l_shipinstruct ` CHAR ( 25 ), ` l_shipmode ` CHAR ( 10 ), ` l_comment ` VARCHAR ( 44 ), ` l_dummy ` VARCHAR ( 10 )); Import Data \u00b6 Suppose your work directory is /home/username/workspace and the tpch1g data is stored in /home/username/workspace/tpch1g . Then, issue the following commands in the MySQL shell to load the data. LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/region/region.tbl' INTO TABLE region FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/nation/nation.tbl' INTO TABLE nation FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/customer/customer.tbl' INTO TABLE customer FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/supplier/supplier.tbl' INTO TABLE supplier FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/part/part.tbl' INTO TABLE part FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/partsupp/partsupp.tbl' INTO TABLE partsupp FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/orders/orders.tbl' INTO TABLE orders FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/lineitem/lineitem.tbl' INTO TABLE lineitem FIELDS TERMINATED BY '|' ; PostgreSQL \u00b6 Create tables \u00b6 Connect to your Postgresql database. $ psql Create a schema for testing. postgres=# create schema tpch1g; postgres=# set search_path to tpch1g; Create empty tables; simply copy and paste the following table definition statements into the PostgreSQL prompt. We will import the data later into these tables. -- nation CREATE TABLE IF NOT EXISTS \"nation\" ( \"n_nationkey\" INT , \"n_name\" CHAR ( 25 ), \"n_regionkey\" INT , \"n_comment\" VARCHAR ( 152 ), \"n_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"n_nationkey\" )); -- region CREATE TABLE IF NOT EXISTS \"region\" ( \"r_regionkey\" INT , \"r_name\" CHAR ( 25 ), \"r_comment\" VARCHAR ( 152 ), \"r_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"r_regionkey\" )); -- supplier CREATE TABLE IF NOT EXISTS \"supplier\" ( \"s_suppkey\" INT , \"s_name\" CHAR ( 25 ), \"s_address\" VARCHAR ( 40 ), \"s_nationkey\" INT , \"s_phone\" CHAR ( 15 ), \"s_acctbal\" DECIMAL ( 15 , 2 ), \"s_comment\" VARCHAR ( 101 ), \"s_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"s_suppkey\" )); -- customer CREATE TABLE IF NOT EXISTS \"customer\" ( \"c_custkey\" INT , \"c_name\" VARCHAR ( 25 ), \"c_address\" VARCHAR ( 40 ), \"c_nationkey\" INT , \"c_phone\" CHAR ( 15 ), \"c_acctbal\" DECIMAL ( 15 , 2 ), \"c_mktsegment\" CHAR ( 10 ), \"c_comment\" VARCHAR ( 117 ), \"c_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"c_custkey\" )); -- part CREATE TABLE IF NOT EXISTS \"part\" ( \"p_partkey\" INT , \"p_name\" VARCHAR ( 55 ), \"p_mfgr\" CHAR ( 25 ), \"p_brand\" CHAR ( 10 ), \"p_type\" VARCHAR ( 25 ), \"p_size\" INT , \"p_container\" CHAR ( 10 ), \"p_retailprice\" DECIMAL ( 15 , 2 ) , \"p_comment\" VARCHAR ( 23 ) , \"p_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"p_partkey\" )); -- partsupp CREATE TABLE IF NOT EXISTS \"partsupp\" ( \"ps_partkey\" INT , \"ps_suppkey\" INT , \"ps_availqty\" INT , \"ps_supplycost\" DECIMAL ( 15 , 2 ), \"ps_comment\" VARCHAR ( 199 ), \"ps_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"ps_partkey\" )); -- orders CREATE TABLE IF NOT EXISTS \"orders\" ( \"o_orderkey\" INT , \"o_custkey\" INT , \"o_orderstatus\" CHAR ( 1 ), \"o_totalprice\" DECIMAL ( 15 , 2 ), \"o_orderdate\" DATE , \"o_orderpriority\" CHAR ( 15 ), \"o_clerk\" CHAR ( 15 ), \"o_shippriority\" INT , \"o_comment\" VARCHAR ( 79 ), \"o_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"o_orderkey\" )); -- lineitem CREATE TABLE IF NOT EXISTS \"lineitem\" ( \"l_orderkey\" INT , \"l_partkey\" INT , \"l_suppkey\" INT , \"l_linenumber\" INT , \"l_quantity\" DECIMAL ( 15 , 2 ), \"l_extendedprice\" DECIMAL ( 15 , 2 ), \"l_discount\" DECIMAL ( 15 , 2 ), \"l_tax\" DECIMAL ( 15 , 2 ), \"l_returnflag\" CHAR ( 1 ), \"l_linestatus\" CHAR ( 1 ), \"l_shipdate\" DATE , \"l_commitdate\" DATE , \"l_receiptdate\" DATE , \"l_shipinstruct\" CHAR ( 25 ), \"l_shipmode\" CHAR ( 10 ), \"l_comment\" VARCHAR ( 44 ), \"l_dummy\" VARCHAR ( 10 )); Import Data \u00b6 Suppose your work directory is /home/username/workspace and the tpch1g data is stored in /home/username/workspace/tpch1g . Then, issue the following commands in the PostgreSQL prompt to load the data. \\c opy \"region\" from '/home/username/workspace/tpch1g/region/region.tbl' DELIMITER '|' CSV ; \\c opy \"nation\" from '/home/username/workspace/tpch1g/nation/nation.tbl' DELIMITER '|' CSV ; \\c opy \"customer\" from '/home/username/workspace/tpch1g/customer/customer.tbl' DELIMITER '|' CSV ; \\c opy \"supplier\" from '/home/username/workspace/tpch1g/supplier/supplier.tbl' DELIMITER '|' CSV ; \\c opy \"part\" from '/home/username/workspace/tpch1g/part/part.tbl' DELIMITER '|' CSV ; \\c opy \"partsupp\" from '/home/username/workspace/tpch1g/partsupp/partsupp.tbl' DELIMITER '|' CSV ; \\c opy \"orders\" from '/home/username/workspace/tpch1g/orders/orders.tbl' DELIMITER '|' CSV ; \\c opy \"lineitem\" from '/home/username/workspace/tpch1g/lineitem/lineitem.tbl' DELIMITER '|' CSV ; Apache Spark \u00b6 Put data to HDFS \u00b6 Use following commands to put data into HDFS. Suppose the tpch1g data is stored in /home/username/workspace/tpch1g and you hope to put your data in /tmp/tpch1g in HDFS. $ hdfs dfs -mkdir -p /tmp/tpch1g/region $ hdfs dfs -mkdir -p /tmp/tpch1g/nation $ hdfs dfs -mkdir -p /tmp/tpch1g/customer $ hdfs dfs -mkdir -p /tmp/tpch1g/supplier $ hdfs dfs -mkdir -p /tmp/tpch1g/part $ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp $ hdfs dfs -mkdir -p /tmp/tpch1g/orders $ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem $ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl /tmp/tpch1g/region $ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl /tmp/tpch1g/nation $ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl /tmp/tpch1g/customer $ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl /tmp/tpch1g/supplier $ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl /tmp/tpch1g/part $ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl /tmp/tpch1g/partsupp $ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl /tmp/tpch1g/orders $ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl /tmp/tpch1g/lineitem If you encounter write permission problem in the next step when creating tables, you can use command $ hdfs dfs -chmod -R 777 /tmp/tpch1g to give full access to your directory. Create table and load data \u00b6 Simply copy and paste following queries to spark to set up TPC-H tables. -- nation CREATE TABLE IF NOT EXISTS nation ( ` n_nationkey ` INT , ` n_name ` CHAR ( 25 ), ` n_regionkey ` INT , ` n_comment ` VARCHAR ( 152 ), ` n_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/nation/nation' ; -- region CREATE TABLE IF NOT EXISTS region ( ` r_regionkey ` INT , ` r_name ` CHAR ( 25 ), ` r_comment ` VARCHAR ( 152 ), ` r_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/region/region' ; -- supplier CREATE TABLE IF NOT EXISTS supplier ( ` s_suppkey ` INT , ` s_name ` CHAR ( 25 ), ` s_address ` VARCHAR ( 40 ), ` s_nationkey ` INT , ` s_phone ` CHAR ( 15 ), ` s_acctbal ` DECIMAL ( 15 , 2 ), ` s_comment ` VARCHAR ( 101 ), ` s_dummy ` varchar ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/supplier/supplier' ; -- customer CREATE TABLE IF NOT EXISTS customer ( ` c_custkey ` INT , ` c_name ` VARCHAR ( 25 ), ` c_address ` VARCHAR ( 40 ), ` c_nationkey ` INT , ` c_phone ` CHAR ( 15 ), ` c_acctbal ` DECIMAL ( 15 , 2 ), ` c_mktsegment ` CHAR ( 10 ), ` c_comment ` VARCHAR ( 117 ), ` c_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/customer/customer' ; -- part CREATE TABLE IF NOT EXISTS part ( ` p_partkey ` INT , ` p_name ` VARCHAR ( 55 ), ` p_mfgr ` CHAR ( 25 ), ` p_brand ` CHAR ( 10 ), ` p_type ` VARCHAR ( 25 ), ` p_size ` INT , ` p_container ` CHAR ( 10 ), ` p_retailprice ` DECIMAL ( 15 , 2 ) , ` p_comment ` VARCHAR ( 23 ) , ` p_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/part/part' ; -- partsupp CREATE TABLE IF NOT EXISTS partsupp ( ` ps_partkey ` INT , ` ps_suppkey ` INT , ` ps_availqty ` INT , ` ps_supplycost ` DECIMAL ( 15 , 2 ), ` ps_comment ` VARCHAR ( 199 ), ` ps_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/partsupp/partsupp' ; -- orders CREATE TABLE IF NOT EXISTS orders ( ` o_orderkey ` INT , ` o_custkey ` INT , ` o_orderstatus ` CHAR ( 1 ), ` o_totalprice ` DECIMAL ( 15 , 2 ), ` o_orderdate ` DATE , ` o_orderpriority ` CHAR ( 15 ), ` o_clerk ` CHAR ( 15 ), ` o_shippriority ` INT , ` o_comment ` VARCHAR ( 79 ), ` o_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/orders/orders' ; -- lineitem CREATE TABLE IF NOT EXISTS lineitem ( ` l_orderkey ` INT , ` l_partkey ` INT , ` l_suppkey ` INT , ` l_linenumber ` INT , ` l_quantity ` DECIMAL ( 15 , 2 ), ` l_extendedprice ` DECIMAL ( 15 , 2 ), ` l_discount ` DECIMAL ( 15 , 2 ), ` l_tax ` DECIMAL ( 15 , 2 ), ` l_returnflag ` CHAR ( 1 ), ` l_linestatus ` CHAR ( 1 ), ` l_shipdate ` DATE , ` l_commitdate ` DATE , ` l_receiptdate ` DATE , ` l_shipinstruct ` CHAR ( 25 ), ` l_shipmode ` CHAR ( 10 ), ` l_comment ` VARCHAR ( 44 ), ` l_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/lineitem/lineitem' ; Redshift \u00b6 Create tables \u00b6 Use SQL query tools like SQL Workbench/J to connect with your Redshift Cluster. First, Create a schema for testing create schema \"tpch1g\" ; Then Create empty tables. -- nation CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"nation\" ( \"n_nationkey\" INT , \"n_name\" CHAR ( 25 ), \"n_regionkey\" INT , \"n_comment\" VARCHAR ( 152 ), \"n_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"n_nationkey\" )); -- region CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"region\" ( \"r_regionkey\" INT , \"r_name\" CHAR ( 25 ), \"r_comment\" VARCHAR ( 152 ), \"r_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"r_regionkey\" )); -- supplier CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"supplier\" ( \"s_suppkey\" INT , \"s_name\" CHAR ( 25 ), \"s_address\" VARCHAR ( 40 ), \"s_nationkey\" INT , \"s_phone\" CHAR ( 15 ), \"s_acctbal\" DECIMAL ( 15 , 2 ), \"s_comment\" VARCHAR ( 101 ), \"s_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"s_suppkey\" )); -- customer CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"customer\" ( \"c_custkey\" INT , \"c_name\" VARCHAR ( 25 ), \"c_address\" VARCHAR ( 40 ), \"c_nationkey\" INT , \"c_phone\" CHAR ( 15 ), \"c_acctbal\" DECIMAL ( 15 , 2 ), \"c_mktsegment\" CHAR ( 10 ), \"c_comment\" VARCHAR ( 117 ), \"c_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"c_custkey\" )); -- part CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"part\" ( \"p_partkey\" INT , \"p_name\" VARCHAR ( 55 ), \"p_mfgr\" CHAR ( 25 ), \"p_brand\" CHAR ( 10 ), \"p_type\" VARCHAR ( 25 ), \"p_size\" INT , \"p_container\" CHAR ( 10 ), \"p_retailprice\" DECIMAL ( 15 , 2 ) , \"p_comment\" VARCHAR ( 23 ) , \"p_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"p_partkey\" )); -- partsupp CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"partsupp\" ( \"ps_partkey\" INT , \"ps_suppkey\" INT , \"ps_availqty\" INT , \"ps_supplycost\" DECIMAL ( 15 , 2 ), \"ps_comment\" VARCHAR ( 199 ), \"ps_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"ps_partkey\" )); -- orders CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"orders\" ( \"o_orderkey\" INT , \"o_custkey\" INT , \"o_orderstatus\" CHAR ( 1 ), \"o_totalprice\" DECIMAL ( 15 , 2 ), \"o_orderdate\" DATE , \"o_orderpriority\" CHAR ( 15 ), \"o_clerk\" CHAR ( 15 ), \"o_shippriority\" INT , \"o_comment\" VARCHAR ( 79 ), \"o_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"o_orderkey\" )); -- lineitem CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"lineitem\" ( \"l_orderkey\" INT , \"l_partkey\" INT , \"l_suppkey\" INT , \"l_linenumber\" INT , \"l_quantity\" DECIMAL ( 15 , 2 ), \"l_extendedprice\" DECIMAL ( 15 , 2 ), \"l_discount\" DECIMAL ( 15 , 2 ), \"l_tax\" DECIMAL ( 15 , 2 ), \"l_returnflag\" CHAR ( 1 ), \"l_linestatus\" CHAR ( 1 ), \"l_shipdate\" DATE , \"l_commitdate\" DATE , \"l_receiptdate\" DATE , \"l_shipinstruct\" CHAR ( 25 ), \"l_shipmode\" CHAR ( 10 ), \"l_comment\" VARCHAR ( 44 ), \"l_dummy\" VARCHAR ( 10 )); Load Data \u00b6 For Redshift, we use Java method to help inserting our data into Redshift tables. schema is the Redshift schema you create your TPC-H tables and table is the table name of TPCH-table, such as nation , region . conn is the Connection class you get from connecting your Redshift cluster using Redshift JDBC driver. Suppose your work directory is /home/username/workspace and the tpch1g data is stored in /home/username/workspace/tpch1g . static void loadRedshiftData ( String schema , String table , Connection conn ) throws IOException { Statement stmt = conn . createStatement (); String concat = \"\" ; File file = new File ( String . format ( \"/home/username/workspace/tpch1g/%s/%s.tbl\" , table , table )); ResultSet columnMeta = stmt . execute ( String . format ( \"select data_type, ordinal_position from INFORMATION_SCHEMA.COLUMNS where table_name='%s' and table_schema='%s'\" , table , schema )); List < Boolean > quotedNeeded = new ArrayList <>(); for ( int i = 0 ; i < columnMeta . getRowCount (); i ++) { quotedNeeded . add ( true ); } while ( columnMeta . next ()) { String columnType = columnMeta . getString ( 1 ); int columnIndex = columnMeta . getInt ( 2 ); if ( columnType . equals ( \"integer\" ) || columnType . equals ( \"numeric\" )) { quotedNeeded . set ( columnIndex - 1 , false ); } } String content = Files . toString ( file , Charsets . UTF_8 ); for ( String row : content . split ( \"\\n\" )) { String [] values = row . split ( \"\\\\|\" ); row = \"\" ; for ( int i = 0 ; i < values . length - 1 ; i ++) { if ( quotedNeeded . get ( i )) { row = row + getQuoted ( values [ i ]) + \",\" ; } else { row = row + values [ i ] + \",\" ; } } row = row + \"''\" ; if ( concat . equals ( \"\" )) { concat = concat + \"(\" + row + \")\" ; } else concat = concat + \",\" + \"(\" + row + \")\" ; } stmt . execute ( String . format ( \"insert into \\\"%s\\\".\\\"%s\\\" values %s\" , schema , table , concat )); } Cloudera Impala \u00b6 Put data to HDFS \u00b6 Use following commands to put data into HDFS. Suppose the tpch1g data is stored in /home/username/workspace/tpch1g and you hope to put your data in /tmp/tpch1g in HDFS. $ hdfs dfs -mkdir -p /tmp/tpch1g/region $ hdfs dfs -mkdir -p /tmp/tpch1g/nation $ hdfs dfs -mkdir -p /tmp/tpch1g/customer $ hdfs dfs -mkdir -p /tmp/tpch1g/supplier $ hdfs dfs -mkdir -p /tmp/tpch1g/part $ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp $ hdfs dfs -mkdir -p /tmp/tpch1g/orders $ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem $ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl /tmp/tpch1g/region $ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl /tmp/tpch1g/nation $ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl /tmp/tpch1g/customer $ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl /tmp/tpch1g/supplier $ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl /tmp/tpch1g/part $ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl /tmp/tpch1g/partsupp $ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl /tmp/tpch1g/orders $ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl /tmp/tpch1g/lineitem If you encounter write permission problem in the next step when creating tables, you can use command $ hdfs dfs -chmod -R 777 /tmp/tpch1g to give full access to your directory. Create tables and load data \u00b6 Connect to Impala. $ impala-shell Create a schema for testing. create schema `tpch1g`; Create tables and load data. Simply copy and paste the following table definition statements into the Impala shell. -- nation CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` nation ` ( ` n_nationkey ` INT , ` n_name ` STRING , ` n_regionkey ` INT , ` n_comment ` STRING , ` n_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/nation' ; -- region CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` region ` ( ` r_regionkey ` INT , ` r_name ` STRING , ` r_comment ` STRING , ` r_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/region' ; -- supplier CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` supplier ` ( ` s_suppkey ` INT , ` s_name ` STRING , ` s_address ` STRING , ` s_nationkey ` INT , ` s_phone ` STRING , ` s_acctbal ` DECIMAL ( 15 , 2 ), ` s_comment ` STRING , ` s_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/supplier' ; -- customer CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` customer ` ( ` c_custkey ` INT , ` c_name ` STRING , ` c_address ` STRING , ` c_nationkey ` INT , ` c_phone ` STRING , ` c_acctbal ` DECIMAL ( 15 , 2 ), ` c_mktsegment ` STRING , ` c_comment ` STRING , ` c_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/customer' ; -- part CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` part ` ( ` p_partkey ` INT , ` p_name ` STRING , ` p_mfgr ` STRING , ` p_brand ` STRING , ` p_type ` STRING , ` p_size ` INT , ` p_container ` STRING , ` p_retailprice ` DECIMAL ( 15 , 2 ) , ` p_comment ` STRING , ` p_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/part' ; -- partsupp CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` partsupp ` ( ` ps_partkey ` INT , ` ps_suppkey ` INT , ` ps_availqty ` INT , ` ps_supplycost ` DECIMAL ( 15 , 2 ), ` ps_comment ` STRING , ` ps_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/partsupp' ; -- orders CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` orders ` ( ` o_orderkey ` INT , ` o_custkey ` INT , ` o_orderstatus ` STRING , ` o_totalprice ` DECIMAL ( 15 , 2 ), ` o_orderdate ` TIMESTAMP , ` o_orderpriority ` STRING , ` o_clerk ` STRING , ` o_shippriority ` INT , ` o_comment ` STRING , ` o_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/orders' ; -- lineitem CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` lineitem ` ( ` l_orderkey ` INT , ` l_partkey ` INT , ` l_suppkey ` INT , ` l_linenumber ` INT , ` l_quantity ` DECIMAL ( 15 , 2 ), ` l_extendedprice ` DECIMAL ( 15 , 2 ), ` l_discount ` DECIMAL ( 15 , 2 ), ` l_tax ` DECIMAL ( 15 , 2 ), ` l_returnflag ` STRING , ` l_linestatus ` STRING , ` l_shipdate ` TIMESTAMP , ` l_commitdate ` TIMESTAMP , ` l_receiptdate ` TIMESTAMP , ` l_shipinstruct ` STRING , ` l_shipmode ` STRING , ` l_comment ` STRING , ` l_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/lineitem' ;","title":"Load TPC-H Data"},{"location":"tutorial/tpch/#tpc-h-data-setup","text":"This is a step-by-step guide for setting up TPC-H data in different databases. This guide will use 1GB data. This guide assumes you have basic knowledge about issuing commands in a terminal application.","title":"TPC-H Data Setup"},{"location":"tutorial/tpch/#download-data","text":"Go to your work directory (say /home/username/workspace ) and download the data: $ cd /home/username/workspace $ curl http://dbgroup-internal.eecs.umich.edu/projects/verdictdb/tpch1g.zip -o tpch1g.zip Unzip the downloaded file: $ unzip tpch1g.zip It will create a new directory named tpch1g under your work directory. The directory contains 8 sub-directories for each of 8 tables.","title":"Download Data"},{"location":"tutorial/tpch/#mysql","text":"","title":"MySQL"},{"location":"tutorial/tpch/#create-tables","text":"Connect to your MySQL database. Make sure you have already added MySQL to your PATH. $ mysql --local-infile -h 127 .0.0.1 -uroot Create a schema for test. (In mysql shell) > create database tpch1g ; > use tpch1g ; Create empty tables; simply copy and paste the following table definition statements into the MySQL shell. We will import the data later into these tables. -- nation CREATE TABLE IF NOT EXISTS tpch1g . nation ( ` n_nationkey ` INT , ` n_name ` CHAR ( 25 ), ` n_regionkey ` INT , ` n_comment ` VARCHAR ( 152 ), ` n_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` n_nationkey ` )); -- region CREATE TABLE IF NOT EXISTS tpch1g . region ( ` r_regionkey ` INT , ` r_name ` CHAR ( 25 ), ` r_comment ` VARCHAR ( 152 ), ` r_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` r_regionkey ` )); -- supplier CREATE TABLE IF NOT EXISTS tpch1g . supplier ( ` s_suppkey ` INT , ` s_name ` CHAR ( 25 ), ` s_address ` VARCHAR ( 40 ), ` s_nationkey ` INT , ` s_phone ` CHAR ( 15 ), ` s_acctbal ` DECIMAL ( 15 , 2 ), ` s_comment ` VARCHAR ( 101 ), ` s_dummy ` varchar ( 10 ), PRIMARY KEY ( ` s_suppkey ` )); -- customer CREATE TABLE IF NOT EXISTS tpch1g . customer ( ` c_custkey ` INT , ` c_name ` VARCHAR ( 25 ), ` c_address ` VARCHAR ( 40 ), ` c_nationkey ` INT , ` c_phone ` CHAR ( 15 ), ` c_acctbal ` DECIMAL ( 15 , 2 ), ` c_mktsegment ` CHAR ( 10 ), ` c_comment ` VARCHAR ( 117 ), ` c_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` c_custkey ` )); -- part CREATE TABLE IF NOT EXISTS tpch1g . part ( ` p_partkey ` INT , ` p_name ` VARCHAR ( 55 ), ` p_mfgr ` CHAR ( 25 ), ` p_brand ` CHAR ( 10 ), ` p_type ` VARCHAR ( 25 ), ` p_size ` INT , ` p_container ` CHAR ( 10 ), ` p_retailprice ` DECIMAL ( 15 , 2 ) , ` p_comment ` VARCHAR ( 23 ) , ` p_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` p_partkey ` )); -- partsupp CREATE TABLE IF NOT EXISTS tpch1g . partsupp ( ` ps_partkey ` INT , ` ps_suppkey ` INT , ` ps_availqty ` INT , ` ps_supplycost ` DECIMAL ( 15 , 2 ), ` ps_comment ` VARCHAR ( 199 ), ` ps_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` ps_partkey ` )); -- orders CREATE TABLE IF NOT EXISTS tpch1g . orders ( ` o_orderkey ` INT , ` o_custkey ` INT , ` o_orderstatus ` CHAR ( 1 ), ` o_totalprice ` DECIMAL ( 15 , 2 ), ` o_orderdate ` DATE , ` o_orderpriority ` CHAR ( 15 ), ` o_clerk ` CHAR ( 15 ), ` o_shippriority ` INT , ` o_comment ` VARCHAR ( 79 ), ` o_dummy ` VARCHAR ( 10 ), PRIMARY KEY ( ` o_orderkey ` )); -- lineitem CREATE TABLE IF NOT EXISTS tpch1g . lineitem ( ` l_orderkey ` INT , ` l_partkey ` INT , ` l_suppkey ` INT , ` l_linenumber ` INT , ` l_quantity ` DECIMAL ( 15 , 2 ), ` l_extendedprice ` DECIMAL ( 15 , 2 ), ` l_discount ` DECIMAL ( 15 , 2 ), ` l_tax ` DECIMAL ( 15 , 2 ), ` l_returnflag ` CHAR ( 1 ), ` l_linestatus ` CHAR ( 1 ), ` l_shipdate ` DATE , ` l_commitdate ` DATE , ` l_receiptdate ` DATE , ` l_shipinstruct ` CHAR ( 25 ), ` l_shipmode ` CHAR ( 10 ), ` l_comment ` VARCHAR ( 44 ), ` l_dummy ` VARCHAR ( 10 ));","title":"Create tables"},{"location":"tutorial/tpch/#import-data","text":"Suppose your work directory is /home/username/workspace and the tpch1g data is stored in /home/username/workspace/tpch1g . Then, issue the following commands in the MySQL shell to load the data. LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/region/region.tbl' INTO TABLE region FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/nation/nation.tbl' INTO TABLE nation FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/customer/customer.tbl' INTO TABLE customer FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/supplier/supplier.tbl' INTO TABLE supplier FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/part/part.tbl' INTO TABLE part FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/partsupp/partsupp.tbl' INTO TABLE partsupp FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/orders/orders.tbl' INTO TABLE orders FIELDS TERMINATED BY '|' ; LOAD DATA LOCAL INFILE '/home/username/workspace/tpch1g/lineitem/lineitem.tbl' INTO TABLE lineitem FIELDS TERMINATED BY '|' ;","title":"Import Data"},{"location":"tutorial/tpch/#postgresql","text":"","title":"PostgreSQL"},{"location":"tutorial/tpch/#create-tables_1","text":"Connect to your Postgresql database. $ psql Create a schema for testing. postgres=# create schema tpch1g; postgres=# set search_path to tpch1g; Create empty tables; simply copy and paste the following table definition statements into the PostgreSQL prompt. We will import the data later into these tables. -- nation CREATE TABLE IF NOT EXISTS \"nation\" ( \"n_nationkey\" INT , \"n_name\" CHAR ( 25 ), \"n_regionkey\" INT , \"n_comment\" VARCHAR ( 152 ), \"n_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"n_nationkey\" )); -- region CREATE TABLE IF NOT EXISTS \"region\" ( \"r_regionkey\" INT , \"r_name\" CHAR ( 25 ), \"r_comment\" VARCHAR ( 152 ), \"r_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"r_regionkey\" )); -- supplier CREATE TABLE IF NOT EXISTS \"supplier\" ( \"s_suppkey\" INT , \"s_name\" CHAR ( 25 ), \"s_address\" VARCHAR ( 40 ), \"s_nationkey\" INT , \"s_phone\" CHAR ( 15 ), \"s_acctbal\" DECIMAL ( 15 , 2 ), \"s_comment\" VARCHAR ( 101 ), \"s_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"s_suppkey\" )); -- customer CREATE TABLE IF NOT EXISTS \"customer\" ( \"c_custkey\" INT , \"c_name\" VARCHAR ( 25 ), \"c_address\" VARCHAR ( 40 ), \"c_nationkey\" INT , \"c_phone\" CHAR ( 15 ), \"c_acctbal\" DECIMAL ( 15 , 2 ), \"c_mktsegment\" CHAR ( 10 ), \"c_comment\" VARCHAR ( 117 ), \"c_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"c_custkey\" )); -- part CREATE TABLE IF NOT EXISTS \"part\" ( \"p_partkey\" INT , \"p_name\" VARCHAR ( 55 ), \"p_mfgr\" CHAR ( 25 ), \"p_brand\" CHAR ( 10 ), \"p_type\" VARCHAR ( 25 ), \"p_size\" INT , \"p_container\" CHAR ( 10 ), \"p_retailprice\" DECIMAL ( 15 , 2 ) , \"p_comment\" VARCHAR ( 23 ) , \"p_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"p_partkey\" )); -- partsupp CREATE TABLE IF NOT EXISTS \"partsupp\" ( \"ps_partkey\" INT , \"ps_suppkey\" INT , \"ps_availqty\" INT , \"ps_supplycost\" DECIMAL ( 15 , 2 ), \"ps_comment\" VARCHAR ( 199 ), \"ps_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"ps_partkey\" )); -- orders CREATE TABLE IF NOT EXISTS \"orders\" ( \"o_orderkey\" INT , \"o_custkey\" INT , \"o_orderstatus\" CHAR ( 1 ), \"o_totalprice\" DECIMAL ( 15 , 2 ), \"o_orderdate\" DATE , \"o_orderpriority\" CHAR ( 15 ), \"o_clerk\" CHAR ( 15 ), \"o_shippriority\" INT , \"o_comment\" VARCHAR ( 79 ), \"o_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"o_orderkey\" )); -- lineitem CREATE TABLE IF NOT EXISTS \"lineitem\" ( \"l_orderkey\" INT , \"l_partkey\" INT , \"l_suppkey\" INT , \"l_linenumber\" INT , \"l_quantity\" DECIMAL ( 15 , 2 ), \"l_extendedprice\" DECIMAL ( 15 , 2 ), \"l_discount\" DECIMAL ( 15 , 2 ), \"l_tax\" DECIMAL ( 15 , 2 ), \"l_returnflag\" CHAR ( 1 ), \"l_linestatus\" CHAR ( 1 ), \"l_shipdate\" DATE , \"l_commitdate\" DATE , \"l_receiptdate\" DATE , \"l_shipinstruct\" CHAR ( 25 ), \"l_shipmode\" CHAR ( 10 ), \"l_comment\" VARCHAR ( 44 ), \"l_dummy\" VARCHAR ( 10 ));","title":"Create tables"},{"location":"tutorial/tpch/#import-data_1","text":"Suppose your work directory is /home/username/workspace and the tpch1g data is stored in /home/username/workspace/tpch1g . Then, issue the following commands in the PostgreSQL prompt to load the data. \\c opy \"region\" from '/home/username/workspace/tpch1g/region/region.tbl' DELIMITER '|' CSV ; \\c opy \"nation\" from '/home/username/workspace/tpch1g/nation/nation.tbl' DELIMITER '|' CSV ; \\c opy \"customer\" from '/home/username/workspace/tpch1g/customer/customer.tbl' DELIMITER '|' CSV ; \\c opy \"supplier\" from '/home/username/workspace/tpch1g/supplier/supplier.tbl' DELIMITER '|' CSV ; \\c opy \"part\" from '/home/username/workspace/tpch1g/part/part.tbl' DELIMITER '|' CSV ; \\c opy \"partsupp\" from '/home/username/workspace/tpch1g/partsupp/partsupp.tbl' DELIMITER '|' CSV ; \\c opy \"orders\" from '/home/username/workspace/tpch1g/orders/orders.tbl' DELIMITER '|' CSV ; \\c opy \"lineitem\" from '/home/username/workspace/tpch1g/lineitem/lineitem.tbl' DELIMITER '|' CSV ;","title":"Import Data"},{"location":"tutorial/tpch/#apache-spark","text":"","title":"Apache Spark"},{"location":"tutorial/tpch/#put-data-to-hdfs","text":"Use following commands to put data into HDFS. Suppose the tpch1g data is stored in /home/username/workspace/tpch1g and you hope to put your data in /tmp/tpch1g in HDFS. $ hdfs dfs -mkdir -p /tmp/tpch1g/region $ hdfs dfs -mkdir -p /tmp/tpch1g/nation $ hdfs dfs -mkdir -p /tmp/tpch1g/customer $ hdfs dfs -mkdir -p /tmp/tpch1g/supplier $ hdfs dfs -mkdir -p /tmp/tpch1g/part $ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp $ hdfs dfs -mkdir -p /tmp/tpch1g/orders $ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem $ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl /tmp/tpch1g/region $ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl /tmp/tpch1g/nation $ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl /tmp/tpch1g/customer $ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl /tmp/tpch1g/supplier $ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl /tmp/tpch1g/part $ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl /tmp/tpch1g/partsupp $ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl /tmp/tpch1g/orders $ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl /tmp/tpch1g/lineitem If you encounter write permission problem in the next step when creating tables, you can use command $ hdfs dfs -chmod -R 777 /tmp/tpch1g to give full access to your directory.","title":"Put data to HDFS"},{"location":"tutorial/tpch/#create-table-and-load-data","text":"Simply copy and paste following queries to spark to set up TPC-H tables. -- nation CREATE TABLE IF NOT EXISTS nation ( ` n_nationkey ` INT , ` n_name ` CHAR ( 25 ), ` n_regionkey ` INT , ` n_comment ` VARCHAR ( 152 ), ` n_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/nation/nation' ; -- region CREATE TABLE IF NOT EXISTS region ( ` r_regionkey ` INT , ` r_name ` CHAR ( 25 ), ` r_comment ` VARCHAR ( 152 ), ` r_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/region/region' ; -- supplier CREATE TABLE IF NOT EXISTS supplier ( ` s_suppkey ` INT , ` s_name ` CHAR ( 25 ), ` s_address ` VARCHAR ( 40 ), ` s_nationkey ` INT , ` s_phone ` CHAR ( 15 ), ` s_acctbal ` DECIMAL ( 15 , 2 ), ` s_comment ` VARCHAR ( 101 ), ` s_dummy ` varchar ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/supplier/supplier' ; -- customer CREATE TABLE IF NOT EXISTS customer ( ` c_custkey ` INT , ` c_name ` VARCHAR ( 25 ), ` c_address ` VARCHAR ( 40 ), ` c_nationkey ` INT , ` c_phone ` CHAR ( 15 ), ` c_acctbal ` DECIMAL ( 15 , 2 ), ` c_mktsegment ` CHAR ( 10 ), ` c_comment ` VARCHAR ( 117 ), ` c_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/customer/customer' ; -- part CREATE TABLE IF NOT EXISTS part ( ` p_partkey ` INT , ` p_name ` VARCHAR ( 55 ), ` p_mfgr ` CHAR ( 25 ), ` p_brand ` CHAR ( 10 ), ` p_type ` VARCHAR ( 25 ), ` p_size ` INT , ` p_container ` CHAR ( 10 ), ` p_retailprice ` DECIMAL ( 15 , 2 ) , ` p_comment ` VARCHAR ( 23 ) , ` p_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/part/part' ; -- partsupp CREATE TABLE IF NOT EXISTS partsupp ( ` ps_partkey ` INT , ` ps_suppkey ` INT , ` ps_availqty ` INT , ` ps_supplycost ` DECIMAL ( 15 , 2 ), ` ps_comment ` VARCHAR ( 199 ), ` ps_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/partsupp/partsupp' ; -- orders CREATE TABLE IF NOT EXISTS orders ( ` o_orderkey ` INT , ` o_custkey ` INT , ` o_orderstatus ` CHAR ( 1 ), ` o_totalprice ` DECIMAL ( 15 , 2 ), ` o_orderdate ` DATE , ` o_orderpriority ` CHAR ( 15 ), ` o_clerk ` CHAR ( 15 ), ` o_shippriority ` INT , ` o_comment ` VARCHAR ( 79 ), ` o_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/orders/orders' ; -- lineitem CREATE TABLE IF NOT EXISTS lineitem ( ` l_orderkey ` INT , ` l_partkey ` INT , ` l_suppkey ` INT , ` l_linenumber ` INT , ` l_quantity ` DECIMAL ( 15 , 2 ), ` l_extendedprice ` DECIMAL ( 15 , 2 ), ` l_discount ` DECIMAL ( 15 , 2 ), ` l_tax ` DECIMAL ( 15 , 2 ), ` l_returnflag ` CHAR ( 1 ), ` l_linestatus ` CHAR ( 1 ), ` l_shipdate ` DATE , ` l_commitdate ` DATE , ` l_receiptdate ` DATE , ` l_shipinstruct ` CHAR ( 25 ), ` l_shipmode ` CHAR ( 10 ), ` l_comment ` VARCHAR ( 44 ), ` l_dummy ` VARCHAR ( 10 )) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tmp/tpch1g/lineitem/lineitem' ;","title":"Create table and load data"},{"location":"tutorial/tpch/#redshift","text":"","title":"Redshift"},{"location":"tutorial/tpch/#create-tables_2","text":"Use SQL query tools like SQL Workbench/J to connect with your Redshift Cluster. First, Create a schema for testing create schema \"tpch1g\" ; Then Create empty tables. -- nation CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"nation\" ( \"n_nationkey\" INT , \"n_name\" CHAR ( 25 ), \"n_regionkey\" INT , \"n_comment\" VARCHAR ( 152 ), \"n_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"n_nationkey\" )); -- region CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"region\" ( \"r_regionkey\" INT , \"r_name\" CHAR ( 25 ), \"r_comment\" VARCHAR ( 152 ), \"r_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"r_regionkey\" )); -- supplier CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"supplier\" ( \"s_suppkey\" INT , \"s_name\" CHAR ( 25 ), \"s_address\" VARCHAR ( 40 ), \"s_nationkey\" INT , \"s_phone\" CHAR ( 15 ), \"s_acctbal\" DECIMAL ( 15 , 2 ), \"s_comment\" VARCHAR ( 101 ), \"s_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"s_suppkey\" )); -- customer CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"customer\" ( \"c_custkey\" INT , \"c_name\" VARCHAR ( 25 ), \"c_address\" VARCHAR ( 40 ), \"c_nationkey\" INT , \"c_phone\" CHAR ( 15 ), \"c_acctbal\" DECIMAL ( 15 , 2 ), \"c_mktsegment\" CHAR ( 10 ), \"c_comment\" VARCHAR ( 117 ), \"c_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"c_custkey\" )); -- part CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"part\" ( \"p_partkey\" INT , \"p_name\" VARCHAR ( 55 ), \"p_mfgr\" CHAR ( 25 ), \"p_brand\" CHAR ( 10 ), \"p_type\" VARCHAR ( 25 ), \"p_size\" INT , \"p_container\" CHAR ( 10 ), \"p_retailprice\" DECIMAL ( 15 , 2 ) , \"p_comment\" VARCHAR ( 23 ) , \"p_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"p_partkey\" )); -- partsupp CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"partsupp\" ( \"ps_partkey\" INT , \"ps_suppkey\" INT , \"ps_availqty\" INT , \"ps_supplycost\" DECIMAL ( 15 , 2 ), \"ps_comment\" VARCHAR ( 199 ), \"ps_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"ps_partkey\" )); -- orders CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"orders\" ( \"o_orderkey\" INT , \"o_custkey\" INT , \"o_orderstatus\" CHAR ( 1 ), \"o_totalprice\" DECIMAL ( 15 , 2 ), \"o_orderdate\" DATE , \"o_orderpriority\" CHAR ( 15 ), \"o_clerk\" CHAR ( 15 ), \"o_shippriority\" INT , \"o_comment\" VARCHAR ( 79 ), \"o_dummy\" VARCHAR ( 10 ), PRIMARY KEY ( \"o_orderkey\" )); -- lineitem CREATE TABLE IF NOT EXISTS \"tpch1g\" . \"lineitem\" ( \"l_orderkey\" INT , \"l_partkey\" INT , \"l_suppkey\" INT , \"l_linenumber\" INT , \"l_quantity\" DECIMAL ( 15 , 2 ), \"l_extendedprice\" DECIMAL ( 15 , 2 ), \"l_discount\" DECIMAL ( 15 , 2 ), \"l_tax\" DECIMAL ( 15 , 2 ), \"l_returnflag\" CHAR ( 1 ), \"l_linestatus\" CHAR ( 1 ), \"l_shipdate\" DATE , \"l_commitdate\" DATE , \"l_receiptdate\" DATE , \"l_shipinstruct\" CHAR ( 25 ), \"l_shipmode\" CHAR ( 10 ), \"l_comment\" VARCHAR ( 44 ), \"l_dummy\" VARCHAR ( 10 ));","title":"Create tables"},{"location":"tutorial/tpch/#load-data","text":"For Redshift, we use Java method to help inserting our data into Redshift tables. schema is the Redshift schema you create your TPC-H tables and table is the table name of TPCH-table, such as nation , region . conn is the Connection class you get from connecting your Redshift cluster using Redshift JDBC driver. Suppose your work directory is /home/username/workspace and the tpch1g data is stored in /home/username/workspace/tpch1g . static void loadRedshiftData ( String schema , String table , Connection conn ) throws IOException { Statement stmt = conn . createStatement (); String concat = \"\" ; File file = new File ( String . format ( \"/home/username/workspace/tpch1g/%s/%s.tbl\" , table , table )); ResultSet columnMeta = stmt . execute ( String . format ( \"select data_type, ordinal_position from INFORMATION_SCHEMA.COLUMNS where table_name='%s' and table_schema='%s'\" , table , schema )); List < Boolean > quotedNeeded = new ArrayList <>(); for ( int i = 0 ; i < columnMeta . getRowCount (); i ++) { quotedNeeded . add ( true ); } while ( columnMeta . next ()) { String columnType = columnMeta . getString ( 1 ); int columnIndex = columnMeta . getInt ( 2 ); if ( columnType . equals ( \"integer\" ) || columnType . equals ( \"numeric\" )) { quotedNeeded . set ( columnIndex - 1 , false ); } } String content = Files . toString ( file , Charsets . UTF_8 ); for ( String row : content . split ( \"\\n\" )) { String [] values = row . split ( \"\\\\|\" ); row = \"\" ; for ( int i = 0 ; i < values . length - 1 ; i ++) { if ( quotedNeeded . get ( i )) { row = row + getQuoted ( values [ i ]) + \",\" ; } else { row = row + values [ i ] + \",\" ; } } row = row + \"''\" ; if ( concat . equals ( \"\" )) { concat = concat + \"(\" + row + \")\" ; } else concat = concat + \",\" + \"(\" + row + \")\" ; } stmt . execute ( String . format ( \"insert into \\\"%s\\\".\\\"%s\\\" values %s\" , schema , table , concat )); }","title":"Load Data"},{"location":"tutorial/tpch/#cloudera-impala","text":"","title":"Cloudera Impala"},{"location":"tutorial/tpch/#put-data-to-hdfs_1","text":"Use following commands to put data into HDFS. Suppose the tpch1g data is stored in /home/username/workspace/tpch1g and you hope to put your data in /tmp/tpch1g in HDFS. $ hdfs dfs -mkdir -p /tmp/tpch1g/region $ hdfs dfs -mkdir -p /tmp/tpch1g/nation $ hdfs dfs -mkdir -p /tmp/tpch1g/customer $ hdfs dfs -mkdir -p /tmp/tpch1g/supplier $ hdfs dfs -mkdir -p /tmp/tpch1g/part $ hdfs dfs -mkdir -p /tmp/tpch1g/partsupp $ hdfs dfs -mkdir -p /tmp/tpch1g/orders $ hdfs dfs -mkdir -p /tmp/tpch1g/lineitem $ hdfs dfs -put /home/username/workspace/tpch1g/region/region.tbl /tmp/tpch1g/region $ hdfs dfs -put /home/username/workspace/tpch1g/nation/nation.tbl /tmp/tpch1g/nation $ hdfs dfs -put /home/username/workspace/tpch1g/customer/customer.tbl /tmp/tpch1g/customer $ hdfs dfs -put /home/username/workspace/tpch1g/supplier/supplier.tbl /tmp/tpch1g/supplier $ hdfs dfs -put /home/username/workspace/tpch1g/part/part.tbl /tmp/tpch1g/part $ hdfs dfs -put /home/username/workspace/tpch1g/partsupp/partsupp.tbl /tmp/tpch1g/partsupp $ hdfs dfs -put /home/username/workspace/tpch1g/orders/orders.tbl /tmp/tpch1g/orders $ hdfs dfs -put /home/username/workspace/tpch1g/lineitem/lineitem.tbl /tmp/tpch1g/lineitem If you encounter write permission problem in the next step when creating tables, you can use command $ hdfs dfs -chmod -R 777 /tmp/tpch1g to give full access to your directory.","title":"Put data to HDFS"},{"location":"tutorial/tpch/#create-tables-and-load-data","text":"Connect to Impala. $ impala-shell Create a schema for testing. create schema `tpch1g`; Create tables and load data. Simply copy and paste the following table definition statements into the Impala shell. -- nation CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` nation ` ( ` n_nationkey ` INT , ` n_name ` STRING , ` n_regionkey ` INT , ` n_comment ` STRING , ` n_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/nation' ; -- region CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` region ` ( ` r_regionkey ` INT , ` r_name ` STRING , ` r_comment ` STRING , ` r_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/region' ; -- supplier CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` supplier ` ( ` s_suppkey ` INT , ` s_name ` STRING , ` s_address ` STRING , ` s_nationkey ` INT , ` s_phone ` STRING , ` s_acctbal ` DECIMAL ( 15 , 2 ), ` s_comment ` STRING , ` s_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/supplier' ; -- customer CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` customer ` ( ` c_custkey ` INT , ` c_name ` STRING , ` c_address ` STRING , ` c_nationkey ` INT , ` c_phone ` STRING , ` c_acctbal ` DECIMAL ( 15 , 2 ), ` c_mktsegment ` STRING , ` c_comment ` STRING , ` c_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/customer' ; -- part CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` part ` ( ` p_partkey ` INT , ` p_name ` STRING , ` p_mfgr ` STRING , ` p_brand ` STRING , ` p_type ` STRING , ` p_size ` INT , ` p_container ` STRING , ` p_retailprice ` DECIMAL ( 15 , 2 ) , ` p_comment ` STRING , ` p_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/part' ; -- partsupp CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` partsupp ` ( ` ps_partkey ` INT , ` ps_suppkey ` INT , ` ps_availqty ` INT , ` ps_supplycost ` DECIMAL ( 15 , 2 ), ` ps_comment ` STRING , ` ps_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/partsupp' ; -- orders CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` orders ` ( ` o_orderkey ` INT , ` o_custkey ` INT , ` o_orderstatus ` STRING , ` o_totalprice ` DECIMAL ( 15 , 2 ), ` o_orderdate ` TIMESTAMP , ` o_orderpriority ` STRING , ` o_clerk ` STRING , ` o_shippriority ` INT , ` o_comment ` STRING , ` o_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/orders' ; -- lineitem CREATE EXTERNAL TABLE IF NOT EXISTS ` tpch1g ` . ` lineitem ` ( ` l_orderkey ` INT , ` l_partkey ` INT , ` l_suppkey ` INT , ` l_linenumber ` INT , ` l_quantity ` DECIMAL ( 15 , 2 ), ` l_extendedprice ` DECIMAL ( 15 , 2 ), ` l_discount ` DECIMAL ( 15 , 2 ), ` l_tax ` DECIMAL ( 15 , 2 ), ` l_returnflag ` STRING , ` l_linestatus ` STRING , ` l_shipdate ` TIMESTAMP , ` l_commitdate ` TIMESTAMP , ` l_receiptdate ` TIMESTAMP , ` l_shipinstruct ` STRING , ` l_shipmode ` STRING , ` l_comment ` STRING , ` l_dummy ` STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LOCATION '/tmp/tpch1g/lineitem' ;","title":"Create tables and load data"},{"location":"tutorial/examples/mysql/","text":"VerdictDB on MySQL \u00b6 On this page, we will demonstrate an example of a Java application with a VerdictDB library that creates a scrambled table for your database, then executes a query that enable VerdictDB to utilize the scrambled table. Here we assume 1) your MySQL database is running and 2) TPC-H data has been loaded into your database following the instructions in the previous Setup TPC-H Data page. Getting VerdictDB Example Application \u00b6 Before we start, you need to get our example Java application that we will be referring to throughout this entire tutorial. To do so, you need git and run the following command in your working directory: $ git clone git@github.com:verdictdb/verdictdb-tutorial.git This will clone the current VerdictDB tutorial application into your_working_directory/verdictdb-tutorial . Move into the verdictdb_on_mysql directory, where the example we will use is located. cd verdictdb_on_mysql Compiling the Application \u00b6 The tutorial application has been set up with Apache Maven and you should be able to build a runnable jar file by running the following command: $ mvn package This will create a runnable jar file under your_working_directory/verdictdb-tutorial/target . Running the Application \u00b6 We included a shell script run.sh in the example application that you can invoke to execute the example application. The script takes 4 arguments: hostname , port , name of database/schema , and command . Creating a Scrambled Table \u00b6 In this tutorial, we are going to create a scrambled table on lineitem table. With the example application, you can create one by simply running the following command (assuming you are running the MySQL database following our Setup MySQL page): $ ./run.sh localhost 3306 tpch1g create Creating a scrambled table for lineitem... Scrambled table for lineitem has been created. Time Taken = 67 s This command executes the following SQL statement via VerdictDB: CREATE SCRAMBLE tpch1g . lineitem_scramble FROM tpch1g . lineitem A corresponding Java source in the example application is following (simplified for brevity): ... Statement stmt = verdictConn . createStatement (); String createQuery = String . format ( \"CREATE SCRAMBLE %s.lineitem_scramble \" + \"FROM %s.lineitem\" , database , database ); System . out . println ( \"Creating a scrambled table for lineitem...\" ); stmt . execute ( createQuery ); stmt . close (); System . out . println ( \"Scrambled table for lineitem has been created.\" ); ... As it is clear from the above snippet, it is just like running SQL statement via any JDBC connection. After the scrambled table is created, VerdictDB will automatically utilize this scrambled table for any query that involves the lineitem table. Running a Sample Query \u00b6 The example application can execute one very simple aggregation query with and without VerdictDB, which lets you to see the performance difference between the two cases. The actual query is as follows: SELECT avg ( l_extendedprice ) FROM tpch1g . lineitem To run this query with/without VerdictDB, you can simply run the following command: $ ./run.sh localhost 3306 tpch1g run Without VerdictDB: average ( l_extendedprice ) = 38255 .138485 Time Taken = 43 s With VerdictDB: average ( l_extendedprice ) = 38219 .8871659 Time Taken = 10 s You can see that VerdictDB achieves more than 4x speedup with a 99.9% accurate result even with a relatively small dataset that we use in this tutorial (i.e., 1GB).","title":"MySQL"},{"location":"tutorial/examples/mysql/#verdictdb-on-mysql","text":"On this page, we will demonstrate an example of a Java application with a VerdictDB library that creates a scrambled table for your database, then executes a query that enable VerdictDB to utilize the scrambled table. Here we assume 1) your MySQL database is running and 2) TPC-H data has been loaded into your database following the instructions in the previous Setup TPC-H Data page.","title":"VerdictDB on MySQL"},{"location":"tutorial/examples/mysql/#getting-verdictdb-example-application","text":"Before we start, you need to get our example Java application that we will be referring to throughout this entire tutorial. To do so, you need git and run the following command in your working directory: $ git clone git@github.com:verdictdb/verdictdb-tutorial.git This will clone the current VerdictDB tutorial application into your_working_directory/verdictdb-tutorial . Move into the verdictdb_on_mysql directory, where the example we will use is located. cd verdictdb_on_mysql","title":"Getting VerdictDB Example Application"},{"location":"tutorial/examples/mysql/#compiling-the-application","text":"The tutorial application has been set up with Apache Maven and you should be able to build a runnable jar file by running the following command: $ mvn package This will create a runnable jar file under your_working_directory/verdictdb-tutorial/target .","title":"Compiling the Application"},{"location":"tutorial/examples/mysql/#running-the-application","text":"We included a shell script run.sh in the example application that you can invoke to execute the example application. The script takes 4 arguments: hostname , port , name of database/schema , and command .","title":"Running the Application"},{"location":"tutorial/examples/mysql/#creating-a-scrambled-table","text":"In this tutorial, we are going to create a scrambled table on lineitem table. With the example application, you can create one by simply running the following command (assuming you are running the MySQL database following our Setup MySQL page): $ ./run.sh localhost 3306 tpch1g create Creating a scrambled table for lineitem... Scrambled table for lineitem has been created. Time Taken = 67 s This command executes the following SQL statement via VerdictDB: CREATE SCRAMBLE tpch1g . lineitem_scramble FROM tpch1g . lineitem A corresponding Java source in the example application is following (simplified for brevity): ... Statement stmt = verdictConn . createStatement (); String createQuery = String . format ( \"CREATE SCRAMBLE %s.lineitem_scramble \" + \"FROM %s.lineitem\" , database , database ); System . out . println ( \"Creating a scrambled table for lineitem...\" ); stmt . execute ( createQuery ); stmt . close (); System . out . println ( \"Scrambled table for lineitem has been created.\" ); ... As it is clear from the above snippet, it is just like running SQL statement via any JDBC connection. After the scrambled table is created, VerdictDB will automatically utilize this scrambled table for any query that involves the lineitem table.","title":"Creating a Scrambled Table"},{"location":"tutorial/examples/mysql/#running-a-sample-query","text":"The example application can execute one very simple aggregation query with and without VerdictDB, which lets you to see the performance difference between the two cases. The actual query is as follows: SELECT avg ( l_extendedprice ) FROM tpch1g . lineitem To run this query with/without VerdictDB, you can simply run the following command: $ ./run.sh localhost 3306 tpch1g run Without VerdictDB: average ( l_extendedprice ) = 38255 .138485 Time Taken = 43 s With VerdictDB: average ( l_extendedprice ) = 38219 .8871659 Time Taken = 10 s You can see that VerdictDB achieves more than 4x speedup with a 99.9% accurate result even with a relatively small dataset that we use in this tutorial (i.e., 1GB).","title":"Running a Sample Query"},{"location":"tutorial/examples/pyverdict/","text":"Pyverdict with Pyspark \u00b6 This is a simple example program of Pyverdict with Pyspark. To run pyspark, hadoop and hive must be installed and properly configured. Install pyverdict \u00b6 pip install pyverdict Start Hadoop server in terminal \u00b6 $HADOOP_HOME /sbin/start-all.sh $HAOOP_HOME is the directory where your Hadoop is installed. To check if the hadoop server have already started, type jps . If you can see Namenode and Datanode in the list, then the server is properly launched. For example, 483 UserClient 2548 3398 SecondaryNameNode 3592 ResourceManager 3161 NameNode 3689 NodeManager 3804 Jps 3262 DataNode Sometimes you may want to restart you hadoop server. You can use: $HADOOP_HOME /sbin/stop-all.sh $HADOOP_HOME /sbin/start-all.sh To format your namenode, use hdfs namenode -format Import packages \u00b6 import pyverdict from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import * from os.path import abspath disable logs \u00b6 sc = SparkContext . getOrCreate () sc . setLogLevel ( \"off\" ) Start a Spark session \u00b6 warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Pyverdict example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () Create Hive Table \u00b6 spark . sql ( \"DROP TABLE IF EXISTS example\" ) spark . sql ( \"CREATE TABLE IF NOT EXISTS example (key INT, value STRING) USING hive\" ) Load data \u00b6 spark . sql ( \"LOAD DATA LOCAL INPATH 'PATH/kv1.txt' INTO TABLE example\" ) Copy spark into verdict \u00b6 verdict = pyverdict . spark ( spark ) Query through pyverdict \u00b6 Now we can query by pyverdict, the syntax is the same as SQL. Here are some examples: 1. count the number of rows. \u00b6 query = \"SELECT count(*) FROM default.example\" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) Output: 1 row(s) in the result (2.084 seconds) c2 0 500 2. select rows ending with 00. \u00b6 query = \"SELECT * FROM default.example WHERE value LIKE '%00' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) Output: 5 row(s) in the result (0.259 seconds) key value 0 100 val_100 1 200 val_200 2 100 val_100 3 400 val_400 4 200 val_200 3. insert a row by pyspark and select it. \u00b6 query = \"INSERT INTO default.example VALUES (999, 'val_999') \" spark . sql ( query ) query = \"SELECT * FROM default.example WHERE value LIKE '%999' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) Output: 1 row(s) in the result (0.142 seconds) key value 0 999 val_999 Drop the table \u00b6 spark . sql ( \"DROP TABLE IF EXISTS default.example\" ) Stop hadoop server in terminal \u00b6 $HADOOP_HOME /sbin/stop-all.sh Complete python code \u00b6 import pyverdict from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import * from os.path import abspath #disable logs sc = SparkContext . getOrCreate () sc . setLogLevel ( \"off\" ) #prepare spark session warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Pyverdict example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () #load data spark . sql ( \"DROP TABLE IF EXISTS example\" ) spark . sql ( \"CREATE TABLE IF NOT EXISTS example (key INT, value STRING) USING hive\" ) spark . sql ( \"LOAD DATA LOCAL INPATH '/Users/jianxinzhang/Documents/Research/VerdictDB/verdictdb-tutorial/pyverdict_on_spark/kv1.txt' INTO TABLE example\" ) #exectue queries verdict = pyverdict . spark ( spark ) query = \"SELECT count(*) FROM default.example\" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) query = \"SELECT * FROM default.example WHERE value LIKE '%00' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) query = \"INSERT INTO default.example VALUES (999, 'val_999') \" spark . sql ( query ) query = \"SELECT * FROM default.example WHERE value LIKE '%999' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) spark . sql ( \"DROP TABLE IF EXISTS default.example\" )","title":"Pyverdict"},{"location":"tutorial/examples/pyverdict/#pyverdict-with-pyspark","text":"This is a simple example program of Pyverdict with Pyspark. To run pyspark, hadoop and hive must be installed and properly configured.","title":"Pyverdict with Pyspark"},{"location":"tutorial/examples/pyverdict/#install-pyverdict","text":"pip install pyverdict","title":"Install pyverdict"},{"location":"tutorial/examples/pyverdict/#start-hadoop-server-in-terminal","text":"$HADOOP_HOME /sbin/start-all.sh $HAOOP_HOME is the directory where your Hadoop is installed. To check if the hadoop server have already started, type jps . If you can see Namenode and Datanode in the list, then the server is properly launched. For example, 483 UserClient 2548 3398 SecondaryNameNode 3592 ResourceManager 3161 NameNode 3689 NodeManager 3804 Jps 3262 DataNode Sometimes you may want to restart you hadoop server. You can use: $HADOOP_HOME /sbin/stop-all.sh $HADOOP_HOME /sbin/start-all.sh To format your namenode, use hdfs namenode -format","title":"Start Hadoop server in terminal"},{"location":"tutorial/examples/pyverdict/#import-packages","text":"import pyverdict from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import * from os.path import abspath","title":"Import packages"},{"location":"tutorial/examples/pyverdict/#disable-logs","text":"sc = SparkContext . getOrCreate () sc . setLogLevel ( \"off\" )","title":"disable logs"},{"location":"tutorial/examples/pyverdict/#start-a-spark-session","text":"warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Pyverdict example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate ()","title":"Start a Spark session"},{"location":"tutorial/examples/pyverdict/#create-hive-table","text":"spark . sql ( \"DROP TABLE IF EXISTS example\" ) spark . sql ( \"CREATE TABLE IF NOT EXISTS example (key INT, value STRING) USING hive\" )","title":"Create Hive Table"},{"location":"tutorial/examples/pyverdict/#load-data","text":"spark . sql ( \"LOAD DATA LOCAL INPATH 'PATH/kv1.txt' INTO TABLE example\" )","title":"Load data"},{"location":"tutorial/examples/pyverdict/#copy-spark-into-verdict","text":"verdict = pyverdict . spark ( spark )","title":"Copy spark into verdict"},{"location":"tutorial/examples/pyverdict/#query-through-pyverdict","text":"Now we can query by pyverdict, the syntax is the same as SQL. Here are some examples:","title":"Query through pyverdict"},{"location":"tutorial/examples/pyverdict/#1-count-the-number-of-rows","text":"query = \"SELECT count(*) FROM default.example\" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) Output: 1 row(s) in the result (2.084 seconds) c2 0 500","title":"1. count the number of rows."},{"location":"tutorial/examples/pyverdict/#2-select-rows-ending-with-00","text":"query = \"SELECT * FROM default.example WHERE value LIKE '%00' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) Output: 5 row(s) in the result (0.259 seconds) key value 0 100 val_100 1 200 val_200 2 100 val_100 3 400 val_400 4 200 val_200","title":"2. select rows ending with 00."},{"location":"tutorial/examples/pyverdict/#3-insert-a-row-by-pyspark-and-select-it","text":"query = \"INSERT INTO default.example VALUES (999, 'val_999') \" spark . sql ( query ) query = \"SELECT * FROM default.example WHERE value LIKE '%999' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) Output: 1 row(s) in the result (0.142 seconds) key value 0 999 val_999","title":"3. insert a row by pyspark and select it."},{"location":"tutorial/examples/pyverdict/#drop-the-table","text":"spark . sql ( \"DROP TABLE IF EXISTS default.example\" )","title":"Drop the table"},{"location":"tutorial/examples/pyverdict/#stop-hadoop-server-in-terminal","text":"$HADOOP_HOME /sbin/stop-all.sh","title":"Stop hadoop server in terminal"},{"location":"tutorial/examples/pyverdict/#complete-python-code","text":"import pyverdict from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql.functions import * from os.path import abspath #disable logs sc = SparkContext . getOrCreate () sc . setLogLevel ( \"off\" ) #prepare spark session warehouse_location = abspath ( 'spark-warehouse' ) spark = SparkSession \\ . builder \\ . appName ( \"Pyverdict example\" ) \\ . config ( \"spark.sql.warehouse.dir\" , warehouse_location ) \\ . enableHiveSupport () \\ . getOrCreate () #load data spark . sql ( \"DROP TABLE IF EXISTS example\" ) spark . sql ( \"CREATE TABLE IF NOT EXISTS example (key INT, value STRING) USING hive\" ) spark . sql ( \"LOAD DATA LOCAL INPATH '/Users/jianxinzhang/Documents/Research/VerdictDB/verdictdb-tutorial/pyverdict_on_spark/kv1.txt' INTO TABLE example\" ) #exectue queries verdict = pyverdict . spark ( spark ) query = \"SELECT count(*) FROM default.example\" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) query = \"SELECT * FROM default.example WHERE value LIKE '%00' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) query = \"INSERT INTO default.example VALUES (999, 'val_999') \" spark . sql ( query ) query = \"SELECT * FROM default.example WHERE value LIKE '%999' \" res = verdict . sql_raw_result ( query ) print ( res . to_df ()) spark . sql ( \"DROP TABLE IF EXISTS default.example\" )","title":"Complete python code"},{"location":"tutorial/examples/spark/","text":"VerdictDB on Apache Spark \u00b6 We will write a simple example program in Scala, the standard programming language for Apache Spark. To compile our example program, the following tools must be installed. sbt: sbt installation guide Create an empty project \u00b6 The following command creates a project that prints out \"hello\". $ sbt new sbt/scala-seed.g8 A minimal Scala project. name [ Scala Seed Project ] : hello-verdict Template applied in ./hello-verdict Move into the project directory: cd hello-verdict . Remove the src/test directory, which we do not need: rm -rf src/test . Configure build setting to use Spark and VerdictDB \u00b6 Add the following line in build.sbt , under the existing import Dependencies._ line. As of the time of writing, the latest version of Apache Spark only supports Scala 2.11. scalaVersion := \"2.11.1\" Also, replace the existing dependency list with libraryDependencies ++= Seq ( scalaTest % Test , \"org.verdictdb\" % \"verdictdb-core\" % \"0.5.5\" , \"org.apache.spark\" %% \"spark-core\" % \"2.3.1\" % \"provided\" , \"org.apache.spark\" %% \"spark-sql\" % \"2.3.1\" % \"provided\" ) This dependency declaration will let the compiler ( sbt in our case) download relevant libraries automatically. Write an example program \u00b6 Edit src/main/scala/example/Hello.scala as follows: package example import org.apache.spark.sql.SparkSession import org.verdictdb.VerdictContext import org.verdictdb.connection.SparkConnection import scala.util.Random object Hello extends App { val spark = SparkSession . builder () . appName ( \"VerdictDB basic example\" ) . enableHiveSupport () . getOrCreate () spark . sparkContext . setLogLevel ( \"ERROR\" ) import spark.implicits._ val verdict = VerdictContext . fromSparkSession ( spark ) // prepare data prepareData ( spark , verdict ) // run a query and print its result val rs = verdict . sql ( \"select count(*) from myschema.sales\" ) rs . printCsv () // simply the following lines will be printed (the actual count value may vary) // c2 // 950.0 def prepareData ( spark : SparkSession , verdict : VerdictContext ) : Unit = { // create a schema and a table spark . sql ( \"DROP SCHEMA IF EXISTS myschema CASCADE\" ) spark . sql ( \"CREATE SCHEMA IF NOT EXISTS myschema\" ) spark . sql ( \"CREATE TABLE IF NOT EXISTS myschema.sales (product string, price double)\" ) // insert 1000 rows val productList = List ( \"milk\" , \"egg\" , \"juice\" ) val rand = new Random () var query = \"INSERT INTO myschema.sales VALUES\" for ( i <- 0 until 1000 ) { val randInt : Int = rand . nextInt ( 3 ) val product : String = productList ( randInt ) val price : Double = ( randInt + 2 ) * 10 + rand . nextInt ( 10 ) if ( i == 0 ) { query = query + f\" (' $product ', $price %.0f)\" } else { query = query + f\", (' $product ', $price %.0f)\" } } spark . sql ( query ) verdict . sql ( \"BYPASS DROP TABLE IF EXISTS myschema.sales_scramble\" ) verdict . sql ( \"BYPASS DROP SCHEMA IF EXISTS verdictdbtemp CASCADE\" ) verdict . sql ( \"BYPASS DROP SCHEMA IF EXISTS verdictdbmeta CASCADE\" ) verdict . sql ( \"CREATE SCRAMBLE myschema.sales_scramble FROM myschema.sales BLOCKSIZE 100\" ) } } Package and Submit \u00b6 $ sbt assembly $ spark-submit target/scala-2.11/Hello-assembly-0.1.0-SNAPSHOT.jar --class example.Hello This example program is available on this public GitHub repository . See the directory verdictdb_on_spark .","title":"Apache Spark"},{"location":"tutorial/examples/spark/#verdictdb-on-apache-spark","text":"We will write a simple example program in Scala, the standard programming language for Apache Spark. To compile our example program, the following tools must be installed. sbt: sbt installation guide","title":"VerdictDB on Apache Spark"},{"location":"tutorial/examples/spark/#create-an-empty-project","text":"The following command creates a project that prints out \"hello\". $ sbt new sbt/scala-seed.g8 A minimal Scala project. name [ Scala Seed Project ] : hello-verdict Template applied in ./hello-verdict Move into the project directory: cd hello-verdict . Remove the src/test directory, which we do not need: rm -rf src/test .","title":"Create an empty project"},{"location":"tutorial/examples/spark/#configure-build-setting-to-use-spark-and-verdictdb","text":"Add the following line in build.sbt , under the existing import Dependencies._ line. As of the time of writing, the latest version of Apache Spark only supports Scala 2.11. scalaVersion := \"2.11.1\" Also, replace the existing dependency list with libraryDependencies ++= Seq ( scalaTest % Test , \"org.verdictdb\" % \"verdictdb-core\" % \"0.5.5\" , \"org.apache.spark\" %% \"spark-core\" % \"2.3.1\" % \"provided\" , \"org.apache.spark\" %% \"spark-sql\" % \"2.3.1\" % \"provided\" ) This dependency declaration will let the compiler ( sbt in our case) download relevant libraries automatically.","title":"Configure build setting to use Spark and VerdictDB"},{"location":"tutorial/examples/spark/#write-an-example-program","text":"Edit src/main/scala/example/Hello.scala as follows: package example import org.apache.spark.sql.SparkSession import org.verdictdb.VerdictContext import org.verdictdb.connection.SparkConnection import scala.util.Random object Hello extends App { val spark = SparkSession . builder () . appName ( \"VerdictDB basic example\" ) . enableHiveSupport () . getOrCreate () spark . sparkContext . setLogLevel ( \"ERROR\" ) import spark.implicits._ val verdict = VerdictContext . fromSparkSession ( spark ) // prepare data prepareData ( spark , verdict ) // run a query and print its result val rs = verdict . sql ( \"select count(*) from myschema.sales\" ) rs . printCsv () // simply the following lines will be printed (the actual count value may vary) // c2 // 950.0 def prepareData ( spark : SparkSession , verdict : VerdictContext ) : Unit = { // create a schema and a table spark . sql ( \"DROP SCHEMA IF EXISTS myschema CASCADE\" ) spark . sql ( \"CREATE SCHEMA IF NOT EXISTS myschema\" ) spark . sql ( \"CREATE TABLE IF NOT EXISTS myschema.sales (product string, price double)\" ) // insert 1000 rows val productList = List ( \"milk\" , \"egg\" , \"juice\" ) val rand = new Random () var query = \"INSERT INTO myschema.sales VALUES\" for ( i <- 0 until 1000 ) { val randInt : Int = rand . nextInt ( 3 ) val product : String = productList ( randInt ) val price : Double = ( randInt + 2 ) * 10 + rand . nextInt ( 10 ) if ( i == 0 ) { query = query + f\" (' $product ', $price %.0f)\" } else { query = query + f\", (' $product ', $price %.0f)\" } } spark . sql ( query ) verdict . sql ( \"BYPASS DROP TABLE IF EXISTS myschema.sales_scramble\" ) verdict . sql ( \"BYPASS DROP SCHEMA IF EXISTS verdictdbtemp CASCADE\" ) verdict . sql ( \"BYPASS DROP SCHEMA IF EXISTS verdictdbmeta CASCADE\" ) verdict . sql ( \"CREATE SCRAMBLE myschema.sales_scramble FROM myschema.sales BLOCKSIZE 100\" ) } }","title":"Write an example program"},{"location":"tutorial/examples/spark/#package-and-submit","text":"$ sbt assembly $ spark-submit target/scala-2.11/Hello-assembly-0.1.0-SNAPSHOT.jar --class example.Hello This example program is available on this public GitHub repository . See the directory verdictdb_on_spark .","title":"Package and Submit"},{"location":"tutorial/setup/mysql/","text":"Setup MySQL / MariaDB \u00b6 If you already have your own MySQL database set up, you can skip this step and proceed with the next step to load TPC-H data. In this tutorial, we will set up the MySQL database using Docker. Docker Installation \u00b6 In this tutorial, we will launch MariaDB (i.e., open-source fork of MySQL), which is essentially equivalent to MySQL (at least for our tutorial), using Docker. Docker is an open-source project for developers to create, deploy and run any application using containers. You can download Docker here , and simply follow their instructions to install it. Start Your Database \u00b6 After you install Docker, you can launch a MySQL/MariaDB instance in a Docker container by running the following command: docker run --rm -d --name verdictdb-mysql -p 127 .0.0.1:3306:3306 \\ -e MYSQL_DATABASE = test -e MYSQL_ALLOW_EMPTY_PASSWORD = yes mariadb:10 This command launches a MySQL/MariaDB instance locally using the default port of 3306. You can access this database as a user 'root' without passwords. Port is already allocated? You may receive the error like blow when starting the docker container. userland proxy: Bind for 0.0.0.0:3306 failed: port is already allocated This error simply means that there is already a process listening on the port 3306, e.g., already existing MySQL instance on your machine. You can either stop/terminate the MySQL instance and run the above docker command again. Or, you can simply use another port, e.g., 3305, to avoid the port conflict as follows: docker run --rm -d --name verdictdb-mysql -p 127 .0.0.1:3305:3306 \\ -e MYSQL_DATABASE = test -e MYSQL_ALLOW_EMPTY_PASSWORD = yes mariadb:10","title":"MySQL"},{"location":"tutorial/setup/mysql/#setup-mysql-mariadb","text":"If you already have your own MySQL database set up, you can skip this step and proceed with the next step to load TPC-H data. In this tutorial, we will set up the MySQL database using Docker.","title":"Setup MySQL / MariaDB"},{"location":"tutorial/setup/mysql/#docker-installation","text":"In this tutorial, we will launch MariaDB (i.e., open-source fork of MySQL), which is essentially equivalent to MySQL (at least for our tutorial), using Docker. Docker is an open-source project for developers to create, deploy and run any application using containers. You can download Docker here , and simply follow their instructions to install it.","title":"Docker Installation"},{"location":"tutorial/setup/mysql/#start-your-database","text":"After you install Docker, you can launch a MySQL/MariaDB instance in a Docker container by running the following command: docker run --rm -d --name verdictdb-mysql -p 127 .0.0.1:3306:3306 \\ -e MYSQL_DATABASE = test -e MYSQL_ALLOW_EMPTY_PASSWORD = yes mariadb:10 This command launches a MySQL/MariaDB instance locally using the default port of 3306. You can access this database as a user 'root' without passwords. Port is already allocated? You may receive the error like blow when starting the docker container. userland proxy: Bind for 0.0.0.0:3306 failed: port is already allocated This error simply means that there is already a process listening on the port 3306, e.g., already existing MySQL instance on your machine. You can either stop/terminate the MySQL instance and run the above docker command again. Or, you can simply use another port, e.g., 3305, to avoid the port conflict as follows: docker run --rm -d --name verdictdb-mysql -p 127 .0.0.1:3305:3306 \\ -e MYSQL_DATABASE = test -e MYSQL_ALLOW_EMPTY_PASSWORD = yes mariadb:10","title":"Start Your Database"},{"location":"tutorial/setup/spark/","text":"Setup Apache Spark \u00b6 Download / Install \u00b6 Download a pre-compiled jar from the official page . At the time of writing, the file for the latest version is spark-2.3.1-bin-hadoop2.7.tgz , which we will use in this article. For other versions, it should suffice to change the version number. After the file is downloaded, move the file to your convenient location, then compress the file with the following command: tar xzf spark-2.3.1-bin-hadoop2.7.tgz Start a Cluster \u00b6 (This section follows the steps in this official guide .) Move into the unarchived directory in the above step, then type the following command: sbin/start-all.sh The Spark cluster should be up and running. Its default status page is http://localhost:8080/ Connection refused error? If you encounter the error when starting the Spark cluster, please refer to this page .","title":"Apache Spark"},{"location":"tutorial/setup/spark/#setup-apache-spark","text":"","title":"Setup Apache Spark"},{"location":"tutorial/setup/spark/#download-install","text":"Download a pre-compiled jar from the official page . At the time of writing, the file for the latest version is spark-2.3.1-bin-hadoop2.7.tgz , which we will use in this article. For other versions, it should suffice to change the version number. After the file is downloaded, move the file to your convenient location, then compress the file with the following command: tar xzf spark-2.3.1-bin-hadoop2.7.tgz","title":"Download / Install"},{"location":"tutorial/setup/spark/#start-a-cluster","text":"(This section follows the steps in this official guide .) Move into the unarchived directory in the above step, then type the following command: sbin/start-all.sh The Spark cluster should be up and running. Its default status page is http://localhost:8080/ Connection refused error? If you encounter the error when starting the Spark cluster, please refer to this page .","title":"Start a Cluster"}]}